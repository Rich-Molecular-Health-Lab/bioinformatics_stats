---
title: "Constructing Metadata Files"
author: "Alicia Rich"
output:
  html_document:
    theme:
      bootswatch: litera
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: "show"
    fig_caption: true
    df_print: paged
params:
  sampleset: "loris"
  seqrun: "hdz18"
                     
---

```{r setup, include = F}
global             <- config::get(config = "default")

library(tidyverse)
library(bslib)
library(htmltools)

source("setup/conflicted.R")
source("setup/default_inputs.R")
source("setup/config_paths.R")
source("setup/knit_engines.R")
```


```{r, include = F}
source("metadata/loris/factors.R")
source("metadata/loris/diet_schedule.R")
```


```{r, include = F}
source("metadata/loris/nutrition.R")
source("metadata/loris/metadata_key.R")
source("metadata/loris/hdz_loris_log.R")
```


# Intro

The purpose of a metadata file is to organize the potential independent or predictor variables for your analysis into a single table with one row per SampleID. Then, when you produce a set of potential dependent or outcome values, you organize those into a similar structure with the SampleIDs organized rowwise to streamline the process of matching predictor variables to outcome variables by SampleID. It's good practice to keep as much of your information in one tidy table as possible so that you can keep pulling from that source to further filter, wrangle and analyze without losing track of different versions and datasets over time. That means you should brainstorm as many possible predictor variables you might use in downstream analysis as possible and organize them into one tidy table where each SampleID is matched to a value for every variable. You will end up ignoring most of these variables as you construct individual tests and visuals later, so consider this simply a rough draft of your information of interest. I am going to do this with the Pygmy Loris dataset for this tutorial. You may have a very different set of variables to organize for your own project.  

## Previous Scripts

Before this, you should use the [`SampleInventory`](https://rich-molecular-health-lab.github.io/read_processing/SampleInventory.html) script to create the file `compilation.tsv`.  

## Other Configuration Settings

### Sampleset in Params

You can use the sampleset setting under params in the header of this script to select which sampleset you will be working with. So long as the same name is used consistently, this should automatically filter for that name (e.g., loris or marmoset). 

### File Paths

Next, you should make sure your `config.yml` file contains the path to locate each of the files you will be using. Below is an example excerpt from my config file. 

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "Show/Hide External File",
        tagList(tags$pre(includeText(here("config.yml"))))
    )
  )
)
```

Note that I also include paths to files that this script will create. If the file is already there, then it will be overwritten, if not, it will be created there. Run the code below to set up your paths from the config file for the working sampleset you identified in the header:

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "Show/Hide External Script",
        tagList(tags$pre(includeText(here(global$setup))))
    )
  )
)
```

### Sequencing Run Lists

The code in the chunk above also generated a list of formatted codes for each available sequencing run to date, separated by taxa/samplesets (currently just for loris and marmoset). Make sure the end number matches the highest integer we have for that sampleset to date.

### Other Setup Scripts

The script that I pasted above sources additional scripts that I run routinely at the start of any work to bring in functions and other inputs with shorter code chunks. You can flip through the text from those scripts below.

```{r, echo = FALSE}
page_fluid(
    accordion(
      title = "Other External Setup Scripts",
      open = FALSE,
      accordion_panel(
        "conflicted.R",
        tagList(tags$pre(includeText("setup/conflicted.R")))
    ),
      accordion_panel(
        "knit_engines.R",
        tagList(tags$pre(includeText("setup/knit_engines.R")))
    ),
      accordion_panel(
        "config_paths.R",
        tagList(tags$pre(includeText("setup/config_paths.R")))
    )
  )
)
```

### External Scripts for Metadata

I also source external scripts to bring in different configurations of metadata to use in this workflow. Again, it just keeps my markdown file tidier while also making it easier for me to run code from those scripts in different files or stages of analysis while ensuring I only need to make changes to the code or data in one location to apply it everywhere. Toggle the panels below if you would like to see the code in those scripts.
  - *Note: I am including the scripts that I use for loris metadata. These may vary for other datasets we use.*

```{r}
page_fluid(
    accordion(
      title = "External Metadata Scripts",
      open = FALSE,
      accordion_panel(
        "Render a Key for Metadata Variables with Descriptions",
        tagList(tags$pre(includeText(here("metadata/loris/metadata_key.R"))))
       ),
      accordion_panel(
        "Nutrition Data from HDZ",
        tagList(tags$pre(includeText(here("metadata/loris/nutrition.R"))))
    ),
      accordion_panel(
        "Dates of Diet Trials and Other States/Events",
        tagList(tags$pre(includeText(here("metadata/loris/hdz_loris_log.R"))))
    ),
      accordion_panel(
        "Formatting Diet Lists into Tables",
        tagList(tags$pre(includeText(here("metadata/loris/diet_tables.R"))))
    ),
      accordion_panel(
        "Assign colors and titles to factors for visualization",
        tagList(tags$pre(includeText(here("metadata/loris/factors.R"))))
       )
    )
  )

```


# Script

## Organizing Dates

The toughest variables to match and wrange in R are often dates, especially when you are dealing with both states (start and end dates for intervals) and events (single dates). For studies like this one based around daily sample collection, I find it easiest to start by populating a blank dataframe with one row per sample collection day. Then I match date-based variables to this dataframe and join my SampleIDs to it.  
  
Once we combine all the pieces together, we will have a metadata table with one row per day for each subject. Then we can match every sample to subject and day using the same table.

```{r}
sample.days <- tibble(CollectionDate = seq.Date(
                                from = ymd(loris$day1), 
                                to   = today(), 
                                by   = "day")
                      ) %>%
  expand(CollectionDate, subject_list) %>%
  mutate(subject = str_to_lower(subject_list), .keep = "unused") %>%
  group_by(subject) %>%
  mutate(study_day = row_number()) %>% ungroup()
```


## Sample Inventory

First I will read in the sample inventory, which is actually already a metadata table itself. Later we will join this to our daily metadata table to create one large dataframe with all our variable scores matched to a sample.

```{r}
samples <- read.table(path$inventories$all_stages, 
                      header = T, 
                      sep = "\t") %>%
            filter(!is.na(CollectionDate)) %>%
  mutate(across(any_of(date.vars),   ~ ymd(.)),
         across(any_of(yn.vars),     ~ str_to_lower(as.character(.))),
         across(any_of(ids),         ~ str_to_lower(.)),
         subject = str_to_lower(Subject), .keep = "unused") %>%
            mutate(identifier = case_when(
              !is.na(SequenceID)                    ~ SequenceID,
              is.na(SequenceID) & !is.na(ExtractID) ~ ExtractID,
              is.na(ExtractID)  & !is.na(SampleID)  ~ SampleID,
              .default = SampleID
            )) %>% relocate(identifier) %>%
  left_join(sample.days, by = join_by(CollectionDate, subject)) %>%
  mutate(subject = fct(subject),
         subj_day = fct(str_glue("{subject}", "_", "{study_day}"))) %>%
  mutate(holding = case_when(
    CollectionDate < new_holding_start$culi   & subject == "culi"   ~ "old",
    CollectionDate < new_holding_start$warble & subject == "warble" ~ "old",
                             .default = "new"))

print(head(samples))

```

### Ordered Sample Lists for Subsetting

```{r}
sample.list <- samples %>% 
               distinct(identifier, subject, CollectionDate) %>%
               arrange(subject, CollectionDate) %>%
               distinct(identifier) %>%
                   map(\(x) as.list(x)) %>%
  list_flatten(name_spec = "")

sample.vecs <- sample.list %>% unlist()

samp.list.culi  <- samples %>% 
                   filter(subject == "culi") %>%
                   distinct(identifier, CollectionDate) %>%
                   arrange(CollectionDate) %>%
                   distinct(identifier) %>%
                   map(\(x) as.list(x)) %>%
  list_flatten(name_spec = "")

samples.culi       <- samp.list.culi %>% unlist()

samp.list.warb  <- samples %>% 
                   filter(subject == "warble") %>%
                   distinct(identifier, CollectionDate) %>%
                   arrange(CollectionDate) %>%
                   distinct(identifier) %>%
                   map(\(x) as.list(x)) %>%
  list_flatten(name_spec = "")

samples.warb <- samp.list.warb %>% unlist()

libraries <- samples %>%
  arrange(LibPrepDate) %>%
  select(LibraryCode, identifier) %>%
  group_by(LibraryCode) %>%
  group_map(~ {
    setNames(list(as.list(.x$identifier)), .y$LibraryCode)
  }, .keep = TRUE) %>%
  flatten()

working_libraries <- libraries %>%
  keep_at(paste0(params$seqrun)) %>%
  list_c()
```

### Subset Versions

I am also going to create a sample table version that I can easily add back into my dataset after I merge technical and biological replicates by subject and day.

```{r}
identifier.key <- samples %>%
  select(subj_day, identifier, SampleID, ExtractID, SequenceID)

write.table(identifier.key, 
            path$metadata$sample_table$identifier, 
            row.names = F, 
            sep       = "\t") 
```



## Construct Variables

### Diet

We have only two subjects in this dataset: Culi and Warble. Culi underwent a series of diet trials that we organized into start and end dates, while Warble maintained a baseline diet throughout our study.  
  
I organized the schedule of diet trials into a nested list based on the day that each new diet plan began. We can use some reproducible code to turn that into a table where each trial end date is the day before the next trial's start date, and then expand longer so that we have one row of data for each day of a diet trial. Later we will match those days to a schedule of sample collection so that each sample is matched to this list of dietary context.  
  
Note that the script I am drawing from for this table also nested more detailed tables into individual columns. This is a way to organize our multi-level nutritional data into a single source that is easier to manipulate and restructure, depending on the scale or variables we want to include. So each row will represent one day, and each column represents a value for one of our variables. The nested columns contain a single table linking that day to that dataset, and additional rows organized by other variables.  

```{r}
nutrients <- read.csv("metadata/loris/nutrients_bydiet.csv", header = TRUE) %>%
  select(diet_name = Diet,
         class     = Nutrient.Class,
         item      = Nutrient,
         fed       = X,
         units     = X.1) %>%
  filter(!is.na(fed)) %>%
  mutate(item = case_when(str_detect(units, "DM") ~ "Dry Matter", 
                          str_detect(units, "kcal") ~"KCal",
                          .default = item),
         fed   = if_else(units == "g" | units == "g as fed" | units == "g DM", fed*1000, fed)) %>%
  mutate(units = case_when(units == "g" | units == "g as fed" | units == "g DM" ~ "mg", 
                           item == "Ca:P" ~ "ratio",
                           .default = units)) %>%
  group_by(class, item) %>%
  mutate(relative    = fed/max(fed),
         relative_to = "item_max") %>% ungroup()

nutrition <- read.csv("metadata/loris/foods_bydiet.csv", header = TRUE) %>%
  mutate(item  = str_to_sentence(Feeds),
         fed   = Fed..g. * 1000,
         class = "Foods",
         units = "mg") %>%
  select(diet_name = Diet,
         class,
         item,
         fed,
         units) %>%
  group_by(diet_name) %>%
  mutate(total_diet = sum(fed)) %>%
  ungroup() %>% rowwise() %>%
  mutate(relative    = fed/total_diet,
         relative_to = "diet_total") %>% ungroup() %>%
  select(-total_diet) %>%
  bind_rows(nutrients)

write.table(nutrition, 
            "metadata/loris/nutrition.tsv", 
            sep       = "\t", 
            row.names = FALSE)
```


```{r}
nutrition_wide <- nutrition %>%
  select(diet_name, class, item, relative) %>%
  mutate(
    across(c(class, item), 
                ~ str_replace_all(.,
              "\\s|:", "_")),
    across(c(class, item), 
                ~ str_remove_all(., "[(),+]"))) %>%
  mutate(item = if_else(item == "Total", 
                        str_glue("{item}", "{class}", .sep = "_"),
                        item)) %>%
  pivot_wider(id_cols     = "diet_name",
              names_from  = "item",
              values_from = "relative",
              values_fill = 0,
              names_sep   = "_") %>%
  relocate(
    ends_with("_Total"),
    starts_with("Total_"), .after = "diet_name") %>%
  rename(Total_rel_max = Total_Total)
```

```{r}
diet <- enframe(diet_trials, name = NULL) %>%
  unnest_wider(value) %>%
  mutate(diet_name = diet, 
         subject   = "culi", .keep = "unused") %>%
  mutate(end = if_else(begin < max(begin), 
                       lead(begin) - days(1), 
                       today())) %>%
  bind_rows(warble_trials) %>%
  mutate(date = map2(begin, end, seq, by = "day")) %>%  
  unnest(date) %>%
  select(-c(begin, end)) %>%
  relocate(date) %>% distinct() %>%
  left_join(nutrition_wide, by = join_by(diet_name))

print(head(diet))
```

Now we will do the same for the list that I created for each medication or supplement administered. Because these all have different dosage units and relative effects, I will adjust the dose units to represent the proportion of the max dosage administered. This makes the data easier to interpret and relate.  
  
Finally, I will join the data on nutrition content, diet schedules, and medications/supplements into a single table with one row for each day.

```{r}
diet_supplements <- enframe(supplements, name = NULL) %>%
  unnest_wider(value) %>%
  mutate(end = if_else(
    begin < max(begin), 
    lead(begin) - days(1), today()),
    subject = "culi") %>%
  bind_rows(warble_supplements) %>%
  pivot_longer(cols      = c(probiotic:antidiarrheal),
               names_to  = "supplement",
               values_to = "dose") %>%
  select(begin, end, subject, supplement, dose) %>%
  group_by(supplement) %>%
  mutate(dose = dose/max(dose)) %>% ungroup() %>%
  pivot_wider(id_cols     = c("begin", "end", "subject"),
              names_from  = "supplement",
              values_from = "dose") %>%
  mutate(date = map2(begin, end, seq, by = "day")) %>%  
  unnest(date) %>%
  select(-c(begin, end)) %>%
  relocate(date) %>%
  right_join(diet, by = join_by(subject, date)) %>% distinct()

print(head(diet_supplements))
```

## Subject Info

Now I will read in some useful info about each subject from their AZA studbook.

```{r, warning = FALSE}
studbook.info <- read.csv(path$metadata$studbook, header = TRUE) %>%
  mutate(
          subject       = factor(str_to_lower(Name), levels = unique(str_to_lower(Name))),
          StudbookID    = factor(ID, levels = unique(ID)),
          Sex           = factor(Sex, levels = unique(Sex)),
          MotherID      = factor(Mother, levels = unique(Mother)),
          FatherID      = factor(Father, levels = unique(Father)),
          BirthLocation = factor(Birth.Location, levels = unique(Birth.Location)),
          BirthYear     = year(make_date(year = Born))
          ) %>%
  select(
    subject,
    StudbookID,
    Sex,
    MotherID,
    FatherID,
    BirthYear,
    BirthLocation
  )
```

## Bristol Scores

```{r}
bristols <- read.table(path$metadata$bristol, 
                       header = TRUE, 
                       sep    = "\t") %>%
  mutate(date = ymd(date))
print(head(bristols))
```


## Complete Metadata Table

```{r}
metadata <- samples %>%
  left_join(studbook.info, by = join_by(subject)) %>%
  mutate(subject_age = year(CollectionDate) - BirthYear) %>% 
  select(-BirthYear) %>%
  left_join(bristols    , by = join_by(subject, CollectionDate == date)) %>%
  left_join(keeper_notes, by = join_by(subject, CollectionDate == date)) %>%
  left_join(cycle_dates , by = join_by(CollectionDate == date)) %>%
  left_join(access_dates, by = join_by(CollectionDate == date)) %>%
  mutate(warb_status = replace_na(warb_status, "anestrus"),
         pair_access = replace_na(pair_access, "n")) %>%
  left_join(diet_supplements, by = join_by(subject, CollectionDate == date)) %>% 
  distinct() %>%
  arrange(study_day) %>%
  group_by(subject) %>%
  fill(bristol_mean) %>%
  ungroup() %>%
  mutate(SampleSet = params$sampleset, bristol = bristol_mean) %>%
  arrange(study_day, subject, identifier) %>%
  select(any_of(meta.vars.ordered))

write.table(metadata, path$metadata$summary , row.names = F, sep = "\t")


print(head(metadata))
```


# Next Steps

>Now you should proceed to the MicroEco Workflow to Begin Merging Metadata with Microbiome Results for Analysis.





