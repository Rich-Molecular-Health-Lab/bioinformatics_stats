---
title: "Loris Microbiome Project Results Overview - IN PROGRESS"
author: "Alicia Rich"
output:
  html_document:
    theme:
      bootswatch: litera
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: "show"
    fig_caption: true
    df_print: paged
params:
  sampleset: "loris"
                     
---


```{r global, message = FALSE}
global             <- config::get(config = "default")

here::i_am("ExploreResults_LorisMicrobiome.Rmd")
source(here::here(global$setup))

for (file in micro$micro_scripts) {
  source(here(file))
}

source(here(path$metadata$key))
source(here(path$metadata$factors))

dataset_dir <- path$microeco$dataset

theme_set(theme_classic())
thematic_rmd()
thematic_on(accent = "#8785B2FF", fg = "black")
```


# Read in and Format Datasets

I have written several custom functions to expedite some of the functions that I reuse multiple times for data sets across different sample and taxon subsets. The loop below will use one of those functions to read the microeco datasets that I built using the [MicroEcoDataPrep](MicroEcoDataPrep.html) script.  
  
This code makes use of the dataset paths referenced in my config.yaml file. To see the full path (relative to the repository base directory), refer to the relative syntax in this script and the config file below.  

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "Show/Hide Config File (config.yml)",
        tagList(tags$pre(includeText("config.yml")))
    ),
      accordion_panel(
        "Show/Hide Variables and Lists Used in Functions",
        tagList(
          withTags(
            ul(
              li(pre("dataset_types")        , br(), print(dataset_types)),
              li(pre("tax_levels")           , br(), print(tax_levels)),
              li(pre("func_levels")          , br(), print(func_levels)),
              li(pre("datasets")             , br(), print(datasets)),
              li(pre("func_levels")          , br(), print(func_levels)),
              li(pre("func_levels")          , br(), print(func_levels)),
              li(pre("diet_factors")         , br(), print(diet_factors)),
              li(pre("warble_cycle_factors") , br(), print(warble_cycle_factors)),
              li(pre("holding_factors")      , br(), print(holding_factors)),
              li(pre("probiotic_factors")    , br(), print(probiotic_factors)),
              li(pre("steroid_factors")      , br(), print(steroid_factors)),
              li(pre("fiber_factors")        , br(), print(fiber_factors)),
              li(pre("antibiotic_factors")   , br(), print(antibiotic_factors)),
              li(pre("antidiarrheal_factors"), br(), print(antidiarrheal_factors)),
              li(pre("sample_table_numeric") , br(), print(sample_table_numeric)),
              li(pre("env_cols")             , br(), print(env_cols))
              )
            )
          )
        )
    )
  )
```



```{r}
# Create an empty list to store all datasets
microtable_datasets <- list()

# Helper function to safely read files
safe_read <- function(file, ...) {
  if (file.exists(file)) {
    return(read.table(file, ...))
  } else {
    warning(paste("File missing:", file))
    return(NULL)
  }
}

# Loop through dataset types (taxonomic and functional)
for (data_type in dataset_types) {
  levels <- if (data_type == "tax") tax_levels else func_levels
  
  for (level in levels) {
    for (dataset in datasets) {
      # Extract dataset path from config
      if (!is.null(dataset_dir[[dataset]][[level]])) {
        directory <- dataset_dir[[dataset]][[level]]
      } else {
        warning(paste("Path not found in config for:", dataset, level))
        next  # Skip this iteration if the path is missing
      }
      
      # Generate dataset name
      prefix       <- substr(level, 1, 3)  # Extract first 3 letters
      dataset_name <- paste0(prefix, ".dataset.", dataset)

      # Read required files
      tax_table   <- safe_read(file.path(directory, "tax_table.tsv")    , sep = "\t", header = TRUE, row.names = 1)
      otu_table   <- safe_read(file.path(directory, "feature_table.tsv"), sep = "\t", header = TRUE, row.names = 1)
      
      # Format metadata for analyses when reading in sample tables
      sample_tab  <- safe_read(file.path(directory, "sample_table.tsv") , sep = "\t", header = TRUE, row.names = 1) %>%
    mutate(subject      = fct(       subject),
           Sex          = fct(       Sex),
           pair_access  = fct(       pair_access),
           diet_name    = fct(       diet_name    , !!!diet_factors),
           warb_status  = fct(       warb_status  , !!!warble_cycle_factors),
           holding      = fct(       holding      , !!!holding_factors),
           probiotic    = fct_recode(probiotic    , !!!probiotic_factors),
           steroid      = fct_recode(steroid      , !!!steroid_factors),
           fiber        = fct_recode(fiber        , !!!fiber_factors),
           antibiotic   = fct_recode(antibiotic   , !!!antibiotic_factors),
           antidiarrheal= fct_recode(antidiarrheal, !!!antidiarrheal_factors),
           accross(any_of(sample_table_numeric), ~as.numeric(.)))

      # Create dataset object
      if (data_type == "tax") {
        phylo_tree <- if (file.exists(file.path(directory, 
                                                "phylo_tree.tre"))) read.tree(file.path(directory, 
                                                                                        "phylo_tree.tre")) else NULL
        rep_fasta  <- if (file.exists(file.path(directory, 
                                                "rep_fasta.fasta"))) read.fasta(file.path(directory, 
                                                                                          "rep_fasta.fasta")) else NULL

        microtable_datasets$tax[[dataset_name]] <- microtable$new(
          sample_table = sample_tab,
          otu_table    = otu_table,
          tax_table    = tax_table,
          phylo_tree   = phylo_tree,
          rep_fasta    = rep_fasta,
          auto_tidy    = TRUE
        )
      } else {
        microtable_datasets$fun[[dataset_name]] <- microtable$new(
          sample_table = sample_tab,
          otu_table    = otu_table,
          tax_table    = tax_table,
          auto_tidy    = TRUE
        )
      }
    }
  }
}

```

## Calculate Basic Statistics for Interpretation

I also created a function to loop through every dataset and calculate our standard metrics needed for downstream analyses (`cal_abund`, `cal_alphadiv`, `cal_betadiv`).

```{r}
# Function to apply microeco operations for standard phylogenetic datasets
cal_basics_tax <- function(dataset) {
  
  dataset$tidy_dataset()
  
  # Compute abundance
  dataset$cal_abund()

  # Filter taxa
  dataset$filter_taxa(
    rel_abund      = methods_16s$loris$min_abund, 
    freq           = methods_16s$loris$min_freq, 
    include_lowest = methods_16s$loris$include_lowest
  )

  # Calculate alpha diversity
  dataset$cal_alphadiv(PD = methods_16s$loris$alpha_pd)

  # Calculate beta diversity
  dataset$cal_betadiv(
    unifrac = methods_16s$loris$unifrac, 
    method  = methods_16s$loris$betadiv
  )

  return(dataset) # Return modified dataset
}

# Function to apply microeco operations for functional profile versions of datasets
cal_basics_fun <- function(dataset) {
  
  dataset$tidy_dataset()
  
  # Compute abundance
  dataset$cal_abund()

  # Filter taxa
  dataset$filter_taxa(
    rel_abund      = methods_16s$loris$min_abund, 
    freq           = methods_16s$loris$min_freq, 
    include_lowest = methods_16s$loris$include_lowest
  )

  # Calculate alpha diversity
  dataset$cal_alphadiv()

  # Calculate beta diversity
  dataset$cal_betadiv(method  = methods_16s$loris$betadiv)

  return(dataset) # Return modified dataset
}

```

```{r, message = FALSE}
# Loop through all phylogenetic datasets and apply functions
microtable_datasets$tax <-  map(microtable_datasets$tax, \(x) cal_basics_tax(x))

# Loop through all functional profiling datasets and apply functions
microtable_datasets$fun <-  map(microtable_datasets$fun, \(x) cal_basics_fun(x))
```

## Flatten Lists of Objects

```{r, message = FALSE}
# Merge and flatten both phylogenetic and functional datasets into single list
microtables <- list_flatten(microtable_datasets, name_spec = "{inner}")

# Create a separate top-level list of only the default phylogenetic datasets
taxatables <- keep_at(microtable_datasets, "tax") %>% list_flatten(name_spec = "{inner}")
```

## Create `trans_env` Objects

>This class is a wrapper for a series of operations associated with environmental measurements, including redundancy analysis, mantel test, correlation analysis and linear fitting.

We will use the `trans_env` class to look at how some of our other metadata variables affect `bristol_mean` or interact more generally. To do so, we first create new objects of this class.

```{r}
new_names <- function(dataset_name) {
  prefix       <- substr(dataset_name,  1,  3) 
  suffix       <- substr(dataset_name, nchar(dataset_name) - 3, nchar(dataset_name))
  return(paste0(prefix, ".env.", suffix))
}

new_env <- function(dataset) {
  trans_env$new(
    dataset  = dataset,
    env_cols = env_cols 
  )
}

env_objects <- imap(microtables, 
                    function(x, idx) {
                      object        <- trans_env$new(dataset = x, env_cols = env_cols)
                      names(object) <- new_names(idx)
                      
                      return(object)
                      })
```

## Unlist All Objects

```{r}
list2env(microtables, envir = .GlobalEnv)
list2env(env_objects, envir = .GlobalEnv)
```

# Differential Abundance of Microbial Taxa

The `trans_diff` function provides several methods for differential abundance testing. The appropriate method depends on the type of data and study design:

  - **Wilcoxon Rank-Sum Test (`wilcox`)**: Non-parametric test for two-group comparisons.
  - **Kruskal-Wallis Test (`KW`)**: Non-parametric test for multiple groups.
  - **Linear Model (`lm`)**: Models taxa abundance as a function of predictor variables.
  - **Beta Regression (`betareg`)**: For proportion data (relative abundance between 0-1).
  - **Generalized Linear Mixed Model (`glmm`)**: Incorporates random effects for repeated measures.
  - **Zero-inflated GLMM (`glmm_beta`)**: Handles zero-inflated taxa abundance data with a beta distribution.
  

## More Info on GLMMs {.tabset}
  
### When to Use GLMMs

1.  **Hierarchical Data**: For example, when samples are nested within subjects, and subjects are nested within treatment groups.
2.  **Overdispersion**: When the variance in the data is greater than expected under a traditional GLM.
3.  **Non-Normal Response Variables**: For example:
  - Binomial data (e.g., presence/absence of a microbial taxon).
  - Poisson data (e.g., counts of microbial reads).

### Structure of a GLMM

A GLMM has the form:  

$g(E(y)) = X\beta + Zb$
 
  
- $g(E(y))$: A link function (e.g., log, logit) that relates the expected value of the response variable y to the linear predictors.
- $X\beta$: Fixed effects (predictors like treatment, time, etc.).
- $Zb$: Random effects (e.g., subject-specific effects).


### Common Applications

- **Microbial Abundance:** Using Poisson or negative binomial distributions to model raw or normalized count data.
- **Presence/Absence:** Modeling binary outcomes for specific taxa (e.g., the presence of a pathogenic species).
- **Diversity Metrics:** Modeling alpha diversity indices as the response variable while accounting for repeated measures or nested study designs.

### How GLMMs Work for Microbiome Data

A GLMM can handle situations like:  

- **Raw Counts**: Model the abundance of a specific taxon across samples using Poisson or negative binomial distributions.
- **Taxa Presence/Absence**: Use a logistic regression model to predict whether a taxon is present or absent based on variables like diet, time, or environmental factors.

### Why GLMMs Are Useful

- They allow us to adjust for random effects, like individual variation between subjects or differences between sequencing runs.
- GLMMs can account for overdispersion, which is common in count data like microbial abundances.

### Choosing Between `betareg` and `glmm_beta`

| Method                  | When to Use                                                                                       | Key Features                                                                 |
|-------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Beta Regression (`betareg`)** | - If you have **independent** samples (no repeated measures). <br>- If your relative abundances **do not** have excessive zeros. | - Models values between (0,1). <br>- Requires a **pseudo count** if you have zeros. |
| **GLMM with Beta (`glmm_beta`)** | - If you have **repeated measures** (same subject over time). <br>- If you have **hierarchical structure** (e.g., multiple samples per subject). <br>- If you have **zero-inflation** (many taxa with 0 abundance). | - Accounts for **random effects** (e.g., `(1|subject)`). <br>- Handles **zero-inflation** better than `betareg` by adding a pseudo count. |


## Hypothesis Testing with Differential Abundances
  
### Diet + Supplements + Medications as Predictors of Microbial Abundance

Based on our data characteristics (relative abundances from long-read 16S sequencing), **beta regression (`betareg`) or generalized linear mixed models with a beta distribution (`glmm_beta`)** are the best choices for differential abundance analysis. Here’s how to decide between them:

#### Recommended Approach for Our Data

Becaues we have:

  - **Relative abundances were pre-calculated (`cal_abund`)**, so values are between 0 and 1.
  - **Longitudinal design** (samples over time for the same subjects).
  - **Increasing zeros at lower taxonomic levels** (e.g., species-level has more zero-abundant taxa).

We should start with `glmm_beta` to account for **repeated measures** and **zero-inflation**.

## Implementing `glmm_beta` in microeco

To model genus-level abundance as a function of diet and supplements while accounting for repeated measures.

```{r}
glmm_genus <- trans_diff$new(
                     dataset      = data,
                     taxa_level   = "Genus",
                     method       = "glmm_beta",
                     formula      = 
"diet_name + probiotic + fiber + steroid + diet_name*probiotic + diet_name*fiber + diet_name*steroid + probiotic*fiber*steroid + diet_name*probiotic*fiber*steroid +  (1|subject) + (1|study_day)",
                     filter_thres = 0.001)
```

### Interpreting Results

The result is stored in:

```{r}
summary(glmm_genus$res_diff)
```


This will include:

  - **Estimate**: Effect size of each factor.
  
  - **P-value and Adjusted P-value (`P.adj`)**: Significance level.
  
  - **Conditional and Marginal R²**: Variance explained by fixed and random effects.

### Handling Common Issues

1.  **Singularity warnings (`Can't compute random effect variances`)**:
  - Ensure `subject` has multiple observations.
  - Consider removing random effects if variance is too low.
    - To remove random effects, we can switch to using `beta_reg`.
  
2.  **Rank-deficiency (`dropping columns from rank-deficient model`)**:
  - Check for collinearity among predictors using `cor()`.
  - Drop highly correlated variables.
  
3.  **Zero-inflation**:
  - Use `glmm_beta` to handle excess zeros, especially in our lower level taxa.
  - If a taxon is mostly absent across samples, set a higher `filter_thres` (e.g., 0.005 instead of 0.001).
  - If `glmm_beta` struggles with excessive zeros, consider `glmmTMB` with zero-inflation.
  
    ```
    glmm_species <- glmmTMB(Species_Abundance ~ diet_name + probiotic + fiber + steroid + (1|subject),
                         data   = species_data,
                         family = beta_family(link="logit"))
    ```
  - You also may try increasing the `beta_pseudo` value beyond the default applied by `glmm_beta`.
    - To avoid errors, `glmm_beta` **adds a small pseudo count to 0s** and adjusts 1s slightly downward using the parameter `beta_pseudo = .Machine$double.eps`.
    - You can adust this to something like `beta_pseudo = 0.001` if `.Machine$double.eps` is too small and the model still struggles.
    
    
## Identifying Microbial Biomarkers as Predictors of Fecal Consistency {.tabset}

Because this is going to require some deep exploratory analysis to search for potentially subtle patterns and interactions between taxa, the following two differential abundance estimators are likely our best options.


### Ordinal Regression (clmm for Ordered Data)

You can fit an **ordinal regression model** in `glmmTMB` using the **cumulative link model (CLM)** approach. The key family function is **`family=ordinal()`**, which allows you to model ordered categorical outcomes.

#### When to use it?

- When `bristol_mean` is a ranked outcome (1-7) rather than a true continuous variable.
- When you want to **account for repeated measures (random effect: (`1|subject`)**).
- When you need **better handling of zero-inflation** (compared to `clmm` in ordinal package).

#### Pros:

- More appropriate for ordinal responses like `bristol_mean`.

#### Cons:

- More complex than `clmm` from the `ordinal` package.

#### Implementation

```{r}
bristol_glmm <- trans_diff$new(
  dataset      = gen.dataset.main, 
  taxa_level   = "Genus", 
  method       = "glmm_beta", 
  formula      = "bristol_mean ~ 
  Genus_Abundance + 
  diet_name + 
  probiotic + 
  fiber + 
  steroid +  
  diet_name*probiotic + 
  diet_name*fiber + 
  diet_name*steroid + 
  probiotic*fiber*steroid + 
  diet_name*probiotic*fiber*steroid +  
  (1|subject) + 
  (1|study_day)", 
  filter_thres = 0.001,
  family       = glmmTMB::ordinal(link = "probit")
)
```

### Random Forest and Machine Learning Approaches

#### When to use it?

- If you want to discover patterns without assuming a linear relationship.
- If you suspect complex interactions between multiple microbes and fecal scores.

#### Pros:

- Can identify key microbial taxa that predict `bristol_mean.`
- Nonparametric (no assumptions about relationships).

#### Cons:

- Harder to interpret individual effect sizes.
- Needs a lot of data to avoid overfitting.

#### Implementation

```{r}
bristol_rf <- trans_diff$new(
  dataset      = gen.dataset.main, 
  taxa_level   = "Genus", 
  method       = "rf", 
  group        = "bristol_mean"
)
```


### Feature Selection Using LEfSe

#### When to use it?

- If you want to identify biomarker taxa strongly associated with fecal scores.

#### Pros:

- Identifies taxa with the strongest differential association with bristol_mean.
- LDA scores indicate effect size.

#### Cons:

- Requires categorical grouping of bristol_mean (e.g., binning into low, medium, high stool scores).

#### Implementation

```{r}
bristol_lefse <- trans_diff$new(
  dataset      = gen.dataset.main, 
  taxa_level   = "Genus", 
  method       = "lefse", 
  group        = "bristol_mean"
)
```




# Exploring Time-Series Analyses

## BEEM as a Suitable Approach {.tabset}

BEEM (**Bayesian inference of Ecological interactions from Microbiome time-series data**) is a computational approach that estimates species interactions and dynamic relationships in microbial communities over time. Unlike beemStatic, which is geared toward cross-sectional datasets, **BEEM** is specifically designed for **time-series microbiome data**, making it a better fit for our study.
  - Daily samples collected from two individuals over one year provide a true time-series dataset, which is ideal for BEEM.
  - **Metadata includes key covariates**: We have multiple independent variables that change over time (e.g., diet, supplements, subject health status, enclosure conditions), which can be incorporated into BEEM to explore how microbiome dynamics are influenced by these factors.
  - **BEEM is designed for non-stationary interactions**: The microbial interactions in our dataset likely change over time due to diet shifts, health interventions, or environmental changes. BEEM models these dynamic interactions rather than assuming a static microbial network.
  
### Some of Our Options with BEEM

1.	**Infer Microbial Interactions Over Time**
  - BEEM fits a **generalized Lotka-Volterra (gLV) model**, which describes how different microbial taxa interact (competition, mutualism, predation).
  - It identifies **key players** (dominant taxa) and how their populations influence each other across time.
2.	**Estimate the Influence of External Factors**
  - BEEM allows incorporation of **time-dependent covariates** (e.g., diet composition, antibiotic use, enclosure conditions) to test their effects on microbiome dynamics.
  - In our case, it could reveal whether changes in **diet composition, supplements, or health events** significantly impact microbial interactions.
3.	**Predict Future Microbiome Dynamics**
  - If we provide a subset of the data, BEEM can **forecast** future microbial shifts based on observed patterns.
4.	**Distinguish Individual-Specific Effects**
  - Since we have two subjects, BEEM could estimate whether **subject-specific factors (e.g., individual health, genetics, or behaviors)** contribute to differences in microbiome dynamics.
  
### Prepare Time-Series Dataset

>For our sample table to work with the trans_ts dataset class:  
>*"Two columns with exact names in sample_table are necessary; one is 'Time', which is the time point and should be the numeric class; the other is 'Rep', which represents the biological replicates and is also numeric class. If no replicates, use 1 to represent 1 replicate."*  

#### Trans_ts Class Conversion

>Note that here I am going to increase our filtering threshold to 0.01. This algorithm can't handle low abundance taxa and zero values as much as others, so this will improve its efficiency.

```{r}
prefixes    <- c("spe", "gen", "fam", "ord", "cla", "phy")
sets       <- c("main", "culi", "warb")
ts.objects <- list()

ts_new <- function(dataset) {
  trans_ts$new(dataset = dataset, filter_thres = 0.01)
}



for (set in sets) {
  for (prefix in prefixes) {

    dataset_name <- paste0(prefix, ".dataset.", set)
    ts_name      <- paste0(prefix, ".ts.", set)
       
    dataset_obj <- taxatables[[dataset_name]]

    if (!is.null(dataset_obj)) {
      ts.objects[[ts_name]] <- ts_new(dataset_obj)  # Store trans_ts object
    } else {
      warning(paste("Dataset object not found for:", dataset_name))
    }
  }
}
```


```{r}
cla.ts.main <- trans_ts$new(dataset = cla.dataset.main, filter_thres = 0.005)
```

```{r}
gen.ts.main <- trans_ts$new(dataset = gen.dataset.main, filter_thres = 0.005)
```



### Predict Biomass {.tabset}

>*This function predicts total microbial biomass using the Expectation-Maximization (EM) algorithm from BEEM. It adjusts for relative abundances and estimates total biomass without requiring absolute quantification.*  
  
**Note: Like the functional redundancy algorithm with Tax4fun2, this chunk can be quite memory-intensive and move slowly. You may want to consider running this as a background job.**

```{r}
phy.ts.main$cal_biomass(ncpu = 6)
```

```{r}
ord.ts.main$cal_biomass(ncpu = 6)
```


```{r}
print(ord.ts.main$res_biomass)  
print(ord.ts.main$res_param)
```

```{r}
cla.ts.main$cal_biomass(ncpu = 6)
```

```{r}
print(cla.ts.main$res_biomass)  
print(cla.ts.main$res_param)
```




### Calculate Network

```{r}
ts_network <- function(dataset) {
  dataset$cal_network()
}

map(ts.objects, \(x) ts_network(x))
```



### Calculate Network Modules

```{r}
ts_module <- function(dataset) {
  dataset$calmodule()
}

map(ts.objects, \(x) ts_module(x))
```

### Save Network

Save network as gexf style, which can be opened by [Gephi](https://gephi.org/).

```{r}
ts_module <- function(dataset, filepath) {
  dataset$save_network(filepath = filepath)
}

imap(ts.objects, \(x, idx) ts_module(x, paste0("visuals/gephi/", idx, ".gexf")))

```

### Plot Network

```{r}
ts_plot_network <- function(dataset) {
  dataset$plot_network(method = "networkD3")
}

map(ts.objects, \(x) ts_plot_network(x))

```


### Plot Taxa Roles

#### Get Node Table

```{r}
ts_node <- function(dataset) {
  dataset$get_node_table()
}

map(ts.objects, \(x) ts_node(x))

```

#### Plot

```{r}
ts_plot_network <- function(dataset) {
  dataset$plot_network(method = "networkD3")
}

map(ts.objects, \(x) ts_plot_network(x))

```



