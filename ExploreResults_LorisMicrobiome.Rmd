---
title: "Loris Microbiome Project Results Overview - IN PROGRESS"
author: "Alicia Rich"
output:
  html_document:
    theme:
      bootswatch: litera
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_folding: "show"
    fig_caption: true
    df_print: paged
params:
  sampleset: "loris"
                     
---


```{r global, message = FALSE}
global             <- config::get(config = "default")

here::i_am("ExploreResults_LorisMicrobiome.Rmd")
source(here::here(global$setup))

for (file in micro$micro_scripts) {
  source(here(file))
}

source(here(path$metadata$key))
source(here(path$metadata$factors))

dataset_dir <- path$microeco$dataset

theme_set(theme_classic())
thematic_rmd()
thematic_on(accent = "#8785B2FF", fg = "black")
```


# Read in Data

I have written several custom functions to expedite some of the functions that I reuse multiple times for data sets across different sample and taxon subsets. The loop below will use one of those functions to read the microeco datasets that I built using the [MicroEcoDataPrep](MicroEcoDataPrep.html) script.  
  
This code makes use of the dataset paths referenced in my config.yaml file. To see the full path (relative to the repository base directory), refer to the relative syntax in this script and the config file below.  

```{r, echo = FALSE}
page_fluid(
    accordion(
      open = FALSE,
      accordion_panel(
        "Show/Hide Config File (config.yml)",
        tagList(tags$pre(includeText("config.yml")))
    )
  )
)
```


```{r}
# Define dataset categories
dataset_types <- c("tax", "function")
tax_levels    <- c("species", "genus", "family", "order", "class", "phylum")
func_levels   <- c("kegg", "fpt", "njc")
datasets      <- c("main", "culi", "warb")

# Create an empty list to store all datasets
microtable_datasets <- list()

# Helper function to safely read files
safe_read <- function(file, ...) {
  if (file.exists(file)) {
    return(read.table(file, ...))
  } else {
    warning(paste("File missing:", file))
    return(NULL)
  }
}

# Loop through dataset types (taxonomic and functional)
for (data_type in dataset_types) {
  levels <- if (data_type == "tax") tax_levels else func_levels
  
  for (level in levels) {
    for (dataset in datasets) {
      # Extract dataset path from config
      if (!is.null(dataset_dir[[dataset]][[level]])) {
        directory <- dataset_dir[[dataset]][[level]]
      } else {
        warning(paste("Path not found in config for:", dataset, level))
        next  # Skip this iteration if the path is missing
      }
      
      # Generate dataset name
      prefix       <- substr(level, 1, 3)  # Extract first 3 letters
      dataset_name <- paste0(prefix, ".dataset.", dataset)

      # Read required files
      sample_tab  <- safe_read(file.path(directory, "sample_table.tsv") , sep = "\t", header = TRUE, row.names = 1)
      tax_table   <- safe_read(file.path(directory, "tax_table.tsv")    , sep = "\t", header = TRUE, row.names = 1)
      otu_table   <- safe_read(file.path(directory, "feature_table.tsv"), sep = "\t", header = TRUE, row.names = 1)

      # Create dataset object
      if (data_type == "tax") {
        phylo_tree <- if (file.exists(file.path(directory, 
                                                "phylo_tree.tre"))) read.tree(file.path(directory, 
                                                                                        "phylo_tree.tre")) else NULL
        rep_fasta  <- if (file.exists(file.path(directory, 
                                                "rep_fasta.fasta"))) read.fasta(file.path(directory, 
                                                                                          "rep_fasta.fasta")) else NULL

        microtable_datasets$tax[[dataset_name]] <- microtable$new(
          sample_table = sample_tab,
          otu_table    = otu_table,
          tax_table    = tax_table,
          phylo_tree   = phylo_tree,
          rep_fasta    = rep_fasta,
          auto_tidy    = TRUE
        )
      } else {
        microtable_datasets$fun[[dataset_name]] <- microtable$new(
          sample_table = sample_tab,
          otu_table    = otu_table,
          tax_table    = tax_table,
          auto_tidy    = TRUE
        )
      }
    }
  }
}

```

# Formatting our Metadata for Modelling

Our metadata table (``gen.dataset.main[["sample_table"]]``) contains a mix of **categorical, numeric, and binary variables**. Below is a function to properly format each column before running any of our regression models.  
  
This function will also add columns to our sample table with more detailed nutritional variables.

## Read in Additional Metadata Variables

```{r}
foods    <- read.table(path$metadata$foods     , header = T, sep = "\t")
proteins <- read.table(path$metadata$proteins  , header = T, sep = "\t")
fats     <- read.table(path$metadata$fats      , header = T, sep = "\t")
CHOs     <- read.table(path$metadata$CHOs      , header = T, sep = "\t")
Ash      <- read.table(path$metadata$Ash       , header = T, sep = "\t")
vitamins <- read.table(path$metadata$vitamins  , header = T, sep = "\t")
```


```{r}
vars_numeric <- c(
"study_day",
"total_mg",
"total_kcal",
"total_mg_dry",
"protein_fed",
"fat_fed",
"CHO_fed",
"mineral_fed",
"bristol_mean"
)

format_vars <- function(sample_table) {
  sample_table <- sample_table %>%
    mutate(subject      = fct(       subject),
           Sex          = fct(       Sex),
           pair_access  = fct(       pair_access),
           diet_name    = fct(       diet_name    , !!!diet_factors),
           warb_status  = fct(       warb_status  , !!!warble_cycle_factors),
           holding      = fct(       holding      , !!!holding_factors),
           probiotic    = fct_recode(probiotic    , !!!probiotic_factors),
           steroid      = fct_recode(steroid      , !!!steroid_factors),
           fiber        = fct_recode(fiber        , !!!fiber_factors),
           antibiotic   = fct_recode(antibiotic   , !!!antibiotic_factors),
           antidiarrheal= fct_recode(antidiarrheal, !!!antidiarrheal_factors),
           accross(any_of(vars_numeric), ~as.numeric(.)))
}
```


## Convert Binary Variables to Factors

## Convert Continuous Variables to Numeric

## Convert Time and Rep to Factors if Needed

## Handle Missing Values

## Verify Data Formatting Before Running any Models

# Calculate Basic Statistics for Interpretation

I also created a function to loop through every dataset and calculate our standard metrics needed for downstream analyses (`cal_abund`, `cal_alphadiv`, `cal_betadiv`).

```{r}
# Function to apply microeco operations for standard phylogenetic datasets
cal_basics_tax <- function(dataset) {
  
  dataset$tidy_dataset()
  
  # Compute abundance
  dataset$cal_abund()

  # Filter taxa
  dataset$filter_taxa(
    rel_abund      = methods_16s$loris$min_abund, 
    freq           = methods_16s$loris$min_freq, 
    include_lowest = methods_16s$loris$include_lowest
  )

  # Calculate alpha diversity
  dataset$cal_alphadiv(PD = methods_16s$loris$alpha_pd)

  # Calculate beta diversity
  dataset$cal_betadiv(
    unifrac = methods_16s$loris$unifrac, 
    method  = methods_16s$loris$betadiv
  )

  return(dataset) # Return modified dataset
}

# Function to apply microeco operations for functional profile versions of datasets
cal_basics_fun <- function(dataset) {
  
  dataset$tidy_dataset()
  
  # Compute abundance
  dataset$cal_abund()

  # Filter taxa
  dataset$filter_taxa(
    rel_abund      = methods_16s$loris$min_abund, 
    freq           = methods_16s$loris$min_freq, 
    include_lowest = methods_16s$loris$include_lowest
  )

  # Calculate alpha diversity
  dataset$cal_alphadiv()

  # Calculate beta diversity
  dataset$cal_betadiv(method  = methods_16s$loris$betadiv)

  return(dataset) # Return modified dataset
}

```

```{r, message = FALSE}
# Loop through all phylogenetic datasets and apply functions
microtable_datasets$tax <-  map(microtable_datasets$tax, \(x) cal_basics_tax(x))

# Loop through all functional profiling datasets and apply functions
microtable_datasets$fun <-  map(microtable_datasets$fun, \(x) cal_basics_fun(x))

# Merge and flatten both phylogenetic and functional datasets into single list
microtables <- list_flatten(microtable_datasets, name_spec = "{inner}")

# Create a separate top-level list of only the default phylogenetic datasets
taxatables <- keep_at(microtable_datasets, "tax") %>% list_flatten(name_spec = "{inner}")
```

# Convert list of all datasets to global environment variables

```{r}
list2env(microtables, envir = .GlobalEnv)
```

# Choosing a Statistical Method

The `` trans_diff `` function provides several methods for differential abundance testing. The appropriate method depends on the type of data and study design:

- **Wilcoxon Rank-Sum Test (`wilcox`)**: Non-parametric test for two-group comparisons.

- **Kruskal-Wallis Test (`KW`)**: Non-parametric test for multiple groups.

- **Linear Model (`lm`)**: Models taxa abundance as a function of predictor variables.

- **Beta Regression (`betareg`)**: For proportion data (relative abundance between 0-1).

- **Generalized Linear Mixed Model (`glmm`)**: Incorporates random effects for repeated measures.

- **Zero-inflated GLMM (`glmm_beta`)**: Handles zero-inflated taxa abundance data with a beta distribution.

Based on our data characteristics (relative abundances from long-read 16S sequencing), **beta regression (`betareg`) or generalized linear mixed models with a beta distribution (`glmm_beta`)** are the best choices for differential abundance analysis. Here’s how to decide between them:

## Choosing Between `betareg` and `glmm_beta`

| Method                  | When to Use                                                                                       | Key Features                                                                 |
|-------------------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| **Beta Regression (`betareg`)** | - If you have **independent** samples (no repeated measures). <br>- If your relative abundances **do not** have excessive zeros. | - Models values between (0,1). <br>- Requires a **pseudo count** if you have zeros. |
| **GLMM with Beta (`glmm_beta`)** | - If you have **repeated measures** (same subject over time). <br>- If you have **hierarchical structure** (e.g., multiple samples per subject). <br>- If you have **zero-inflation** (many taxa with 0 abundance). | - Accounts for **random effects** (e.g., `(1|subject)`). <br>- Handles **zero-inflation** better than `betareg` by adding a pseudo count. |

## Recommended Approach for Our Data

Becaues we have:

  - **Relative abundances were pre-calculated (`cal_abund`)**, so values are between 0 and 1.
  
  - **Longitudinal design** (samples over time for the same subjects).
  
  - **Increasing zeros at lower taxonomic levels** (e.g., species-level has more zero-abundant taxa).

We should start with `glmm_beta` to account for **repeated measures** and **zero-inflation**.

## Implementing `glmm_beta` in microeco

To model genus-level abundance as a function of diet and supplements while accounting for repeated measures.

```{r}
glmm_genus <- trans_diff$new(
                     dataset      = data,
                     taxa_level   = "Genus",
                     method       = "glmm_beta",
                     formula      = "diet_name + probiotic + fiber + steroid + (1|subject)",
                     filter_thres = 0.001)
```

### Interpreting Results

The result is stored in:

```{r}
summary(glmm_genus$res_diff)
```


This will include:

  - **Estimate**: Effect size of each factor.
  
  - **P-value and Adjusted P-value (`P.adj`)**: Significance level.
  
  - **Conditional and Marginal R²**: Variance explained by fixed and random effects.

### Handling Common Issues

1.  **Singularity warnings (`Can't compute random effect variances`)**:
  - Ensure `subject` has multiple observations.
  - Consider removing random effects if variance is too low.
    - To remove random effects, we can switch to using `beta_reg`.
  
2.  **Rank-deficiency (`dropping columns from rank-deficient model`)**:
  - Check for collinearity among predictors using `cor()`.
  - Drop highly correlated variables.
  
3.  **Zero-inflation**:
  - Use `glmm_beta` to handle excess zeros, especially in our lower level taxa.
  - If a taxon is mostly absent across samples, set a higher `filter_thres` (e.g., 0.005 instead of 0.001).
  - If `glmm_beta` struggles with excessive zeros, consider `glmmTMB` with zero-inflation.
  
    ```
    glmm_species <- glmmTMB(Species_Abundance ~ diet_name + probiotic + fiber + steroid + (1|subject),
                         data   = species_data,
                         family = beta_family(link="logit"))
    ```
  - You also may try increasing the `beta_pseudo` value beyond the default applied by `glmm_beta`.
    - To avoid errors, `glmm_beta` **adds a small pseudo count to 0s** and adjusts 1s slightly downward using the parameter `beta_pseudo = .Machine$double.eps`.
    - You can adust this to something like `beta_pseudo = 0.001` if `.Machine$double.eps` is too small and the model still struggles.

# Linear Models

## Independent Variables

```{r}
diet_vars <- list(
"diet_name"	,
"total_mg"	,
"total_kcal"	,
"total_mg_dry"	,
"protein_fed"	,
"fat_fed"	,
"CHO_fed"	,
"mineral_fed"
)

meds_supplements <- list(
"probiotic"	,
"fiber"	,
"steroid"	,
"antibiotic"	,
"antidiarrheal"	
)

random_effects <- list(
"study_day"	,
"subject"	,
"holding"	,
"pair_access"	,
"warb_status"	
)
```


## Dependent Variables

```{r}
inflammation <- list("bristol_mean")
abundance    <- list("rel_abundance")
alpha_diversity <- list()
```


## Generalized Linear Mixed Models (GLMMs) {.tabset}

GLMMs are an extension of generalized linear models (GLMs), allowing for the inclusion of random effects to account for hierarchical or nested data structures.  

### Designing and Running the Model

```{r}
effects_fixed_diet <- c(
                      "diet_name", 
                      "probiotic", 
                      "fiber", 
                      "steroid", 
                      "total_mg", 
                      "total_kcal")

effects_random <- c("subject")
```

```{r}
null_gen <- trans_nullmodel$new(
  dataset      = gen.dataset.main,
  filter_thres = 0.001,
  env_cols     = c(effects_fixed_diet, effects_random)
)
```

```{r}
null_gen$cal_mantel_corr(use_env = "diet_name")
```

```{r}
null_gen$plot_mantel_corr()
```

```{r}
null_gen$cal_ses_betampd()
```

```{r}
# add betaNRI matrix to beta_diversity list
gen.dataset.main$beta_diversity[["betaNRI"]] <- null_gen$res_ses_betampd
```


```{r}
# create trans_beta class, use measure "betaNRI"
beta_gen <- trans_beta$new(dataset = gen.dataset.main, 
                           group   = "diet_name", 
                           measure = "betaNRI")
```


```{r}
# transform the distance for each group
beta_gen$cal_group_distance()
```


```{r}
# see the help document for more methods, e.g. "anova" and "KW_dunn"
beta_gen$cal_group_distance_diff(method = "wilcox")
```


```{r}
# plot the results
plot_beta_gen <- t2$plot_group_distance(add = "mean")
g1 + geom_hline(yintercept = -2, linetype = 2) + geom_hline(yintercept = 2, linetype = 2)
```

```{r}
mod3 <- lm(body_mass_g ~ bill_length_mm + species, penguins)
extract_eq(mod3)
```


```{r}
glmm_gen <- trans_diff$new(
                     dataset      = gen.dataset.main, 
                     method       = "glmm_beta", 
                     formula      = "diet_name + probiotic + steroid + (1|study_day)", 
                     beta_pseudo  = 1,
                     filter_thres = 0.001)

summary(glmm_gen)
```



### Background
  
#### When to Use GLMMs

1.  **Hierarchical Data**: For example, when samples are nested within subjects, and subjects are nested within treatment groups.
2.  **Overdispersion**: When the variance in the data is greater than expected under a traditional GLM.
3.  **Non-Normal Response Variables**: For example:
  - Binomial data (e.g., presence/absence of a microbial taxon).
  - Poisson data (e.g., counts of microbial reads).

### Structure of a GLMM

A GLMM has the form:  

$g(E(y)) = X\beta + Zb$
 
  
- $g(E(y))$: A link function (e.g., log, logit) that relates the expected value of the response variable y to the linear predictors.
- $X\beta$: Fixed effects (predictors like treatment, time, etc.).
- $Zb$: Random effects (e.g., subject-specific effects).


#### Common Applications

- **Microbial Abundance:** Using Poisson or negative binomial distributions to model raw or normalized count data.
- **Presence/Absence:** Modeling binary outcomes for specific taxa (e.g., the presence of a pathogenic species).
- **Diversity Metrics:** Modeling alpha diversity indices as the response variable while accounting for repeated measures or nested study designs.

#### How GLMMs Work for Microbiome Data

A GLMM can handle situations like:  

- **Raw Counts**: Model the abundance of a specific taxon across samples using Poisson or negative binomial distributions.
- **Taxa Presence/Absence**: Use a logistic regression model to predict whether a taxon is present or absent based on variables like diet, time, or environmental factors.

#### Why GLMMs Are Useful

- They allow us to adjust for random effects, like individual variation between subjects or differences between sequencing runs.
- GLMMs can account for overdispersion, which is common in count data like microbial abundances.


# Exploring Time-Series Analyses

## BEEM as a Suitable Approach {.tabset}

BEEM (**Bayesian inference of Ecological interactions from Microbiome time-series data**) is a computational approach that estimates species interactions and dynamic relationships in microbial communities over time. Unlike beemStatic, which is geared toward cross-sectional datasets, **BEEM** is specifically designed for **time-series microbiome data**, making it a better fit for our study.
  - Daily samples collected from two individuals over one year provide a true time-series dataset, which is ideal for BEEM.
  - **Metadata includes key covariates**: We have multiple independent variables that change over time (e.g., diet, supplements, subject health status, enclosure conditions), which can be incorporated into BEEM to explore how microbiome dynamics are influenced by these factors.
  - **BEEM is designed for non-stationary interactions**: The microbial interactions in our dataset likely change over time due to diet shifts, health interventions, or environmental changes. BEEM models these dynamic interactions rather than assuming a static microbial network.
  
### Some of Our Options with BEEM

1.	**Infer Microbial Interactions Over Time**
  - BEEM fits a **generalized Lotka-Volterra (gLV) model**, which describes how different microbial taxa interact (competition, mutualism, predation).
  - It identifies **key players** (dominant taxa) and how their populations influence each other across time.
2.	**Estimate the Influence of External Factors**
  - BEEM allows incorporation of **time-dependent covariates** (e.g., diet composition, antibiotic use, enclosure conditions) to test their effects on microbiome dynamics.
  - In our case, it could reveal whether changes in **diet composition, supplements, or health events** significantly impact microbial interactions.
3.	**Predict Future Microbiome Dynamics**
  - If we provide a subset of the data, BEEM can **forecast** future microbial shifts based on observed patterns.
4.	**Distinguish Individual-Specific Effects**
  - Since we have two subjects, BEEM could estimate whether **subject-specific factors (e.g., individual health, genetics, or behaviors)** contribute to differences in microbiome dynamics.
  
### Prepare Time-Series Dataset

>For our sample table to work with the trans_ts dataset class:  
>*"Two columns with exact names in sample_table are necessary; one is 'Time', which is the time point and should be the numeric class; the other is 'Rep', which represents the biological replicates and is also numeric class. If no replicates, use 1 to represent 1 replicate."*  

#### Trans_ts Class Conversion

>Note that here I am going to increase our filtering threshold to 0.01. This algorithm can't handle low abundance taxa and zero values as much as others, so this will improve its efficiency.

```{r}
prefixes    <- c("spe", "gen", "fam", "ord", "cla", "phy")
sets       <- c("main", "culi", "warb")
ts.objects <- list()

ts_new <- function(dataset) {
  trans_ts$new(dataset = dataset, filter_thres = 0.01)
}



for (set in sets) {
  for (prefix in prefixes) {

    dataset_name <- paste0(prefix, ".dataset.", set)
    ts_name      <- paste0(prefix, ".ts.", set)
       
    dataset_obj <- taxatables[[dataset_name]]

    if (!is.null(dataset_obj)) {
      ts.objects[[ts_name]] <- ts_new(dataset_obj)  # Store trans_ts object
    } else {
      warning(paste("Dataset object not found for:", dataset_name))
    }
  }
}
```


```{r}
cla.ts.main <- trans_ts$new(dataset = cla.dataset.main, filter_thres = 0.005)
```

```{r}
gen.ts.main <- trans_ts$new(dataset = gen.dataset.main, filter_thres = 0.005)
```



### Predict Biomass {.tabset}

>*This function predicts total microbial biomass using the Expectation-Maximization (EM) algorithm from BEEM. It adjusts for relative abundances and estimates total biomass without requiring absolute quantification.*  
  
**Note: Like the functional redundancy algorithm with Tax4fun2, this chunk can be quite memory-intensive and move slowly. You may want to consider running this as a background job.**

```{r}
phy.ts.main$cal_biomass(ncpu = 6)
```

```{r}
ord.ts.main$cal_biomass(ncpu = 6)
```


```{r}
print(ord.ts.main$res_biomass)  
print(ord.ts.main$res_param)
```

```{r}
cla.ts.main$cal_biomass(ncpu = 6)
```

```{r}
print(cla.ts.main$res_biomass)  
print(cla.ts.main$res_param)
```




### Calculate Network

```{r}
ts_network <- function(dataset) {
  dataset$cal_network()
}

map(ts.objects, \(x) ts_network(x))
```



### Calculate Network Modules

```{r}
ts_module <- function(dataset) {
  dataset$calmodule()
}

map(ts.objects, \(x) ts_module(x))
```

### Save Network

Save network as gexf style, which can be opened by [Gephi](https://gephi.org/).

```{r}
ts_module <- function(dataset, filepath) {
  dataset$save_network(filepath = filepath)
}

imap(ts.objects, \(x, idx) ts_module(x, paste0("visuals/gephi/", idx, ".gexf")))

```

### Plot Network

```{r}
ts_plot_network <- function(dataset) {
  dataset$plot_network(method = "networkD3")
}

map(ts.objects, \(x) ts_plot_network(x))

```


### Plot Taxa Roles

#### Get Node Table

```{r}
ts_node <- function(dataset) {
  dataset$get_node_table()
}

map(ts.objects, \(x) ts_node(x))

```

#### Plot

```{r}
ts_plot_network <- function(dataset) {
  dataset$plot_network(method = "networkD3")
}

map(ts.objects, \(x) ts_plot_network(x))

```



