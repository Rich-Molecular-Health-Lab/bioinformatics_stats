---
title: "Rich Lab - Process MinION 16S Reads"
author: "Alicia Rich"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    css: "microbiome.css"
    toc: true
    toc_location: "before"
    toc_depth: 4
    number_sections: false
    toc_float: true
    code_folding: "hide"
    fig_caption: true
    bibliography: ["/Users/aliciamrich/RStudioMacbook/Bibliographies/loris_mb.bib"]
  link_citations: true
  csl: ["/Users/aliciamrich/RStudioMacbook/Bibliographies/plos.csl"]
params:
  date: !r Sys.Date()
  day1: "2023-10-25"
  seqrun: hdz16
  min_length: 1000
  max_length: 2000
  min_qual: 7
  min_id: 85
  min_cov: 80
  ntaxa: 12
  kit_name: "SQK-16S114-24"
  model: dna_r10.4.1_e8.2_400bps_sup@v5.0.0
  work_dir: "/work/richlab/aliciarich/microbiomes_loris"
  local: "/Users/aliciamrich/RStudioMacbook/GitRepos/microbiomes_loris/"
  laptop: "/Users/aliciamrich/RStudioMacbook/GitRepos/microbiomes_loris/"
  desktop: "/Users/arich/Library/CloudStorage/GoogleDrive-aliciamrich@gmail.com/Other computers/My MacBook Pro/RStudioMacbook/GitRepos/microbiomes_loris/"
---

```{r include=FALSE}
source(paste0(params$local, "dependencies/R/setup.R"))
```


# Intro {.tabset}

## This Workflow

>This is the current recommended pipeline for processing raw reads obtained from the MinION Sequencer. This workflow assumes you have used some combination of ONT barcoding kits and primers to sequence the entire ~1500 bp 16S gene. Once you finish your run on the MinION sequencer, you will take the pod5 files and re-basecall them before running them through additional workflows below. Once you finish this pipeline, you should continue with another markdown file created specifically for the back-end analysis that we run primarily in R, where you will generate some summary and inferential statistics and reproting-quality visuals for the data.

>This script will rely primarily on the computational power of the HCC's Swan cluster, but some of the steps can also be run locally on the mac mini computers in the lab or your personal laptop, depending on how powerful your system is and how much time you are willing to wait for the programs to complete.

>If you see a step involving the code "sbatch", that means I am referencing a separate file with the extension .sh as a complete batch script that runs from the HCC's SLURM server. You should transfer your version of that script to the local working directory before running the sbatch code.
  
## Note on Using Bash Chunks Here
>This script alternates between R and Bash languages in different chunks. 
>If you click the "run chunk" button (sideways triangle in the upper right corner of the chunk) on a chunk marked as r, it will run that chunk in your R console.   
>If you click the "run chunk" button on a chunk marked as swan, it will only print the formatted code for you to copy and paste into the terminal shell where you are logged into your working HCC Swan directory.  

## Interactive vs. Batch Options
>You may opt to complete each step through an interactive job, which is often faster and easier for troubleshooting, or by submitting a batch script, which ensures any disconnection on your part won't interrupt the job and provides a more detailed output file to refer to later.  
>I also include an extra streamlined option below to submit a sequential series of batch scripts that will automatically complete all the HPC tasks in this pipeline. You just have to upload all the scripts and files needed before submitting.  
> -The scripts are all attached below. You will need to run through the sample table generation below and transfer that to the working directory yourself.  
> -You will also need to create all conda environments the first time you run this pipeline.  
>   *see below*


# Setting Up

## First Time Only  {.tabset}
>These are steps you only need to take the first time that you run this pipeline from your working directory.  

### Download Dorado Model
>For some reason dorado's automatic sourcing and use of models does not seem to work from the GPU nodes on the HCC, so we will download a stable version of our current wording model.
>This file needs to be in your working directory where you run the dorado basecaller command or script.

```{swan}
cd "/work/richlab/aliciarich/ont_reads/"

dorado download ${params$model}
```

## Create Conda Environments

```{swan}
module purge
module load anaconda
```

### pycoQC

```{swan}
conda create -n pycoQC python=3.6
conda activate pycoQC
mamba install pycoqc
conda deactivate
```

### filter

```{swan filter env}
module purge
module load anaconda
conda create -n filter
conda activate filter
mamba install chopper
conda deactivate
```

## New Sequencing Run
>These are steps to take each time you begin processing a newly sequenced run

### Update Sample Sheets

>There is another workflow for formatting and exporting an updated master spreadsheet with sample information that you will need for this processing script and the post-processing of metadata. Make sure you have followed that before importing and manipulating the master sheet.
>You may want to export a sample sheet for one run, multiple runs, or all sequencing runs, depending on which data you plan to basecall, demultiplex, and classify below. Import the master sheet and then filter for whichever subset you want before proceeding.

```{r}
sample.sheet <- read.recent.version.tsv("dataframes", "master_sample_sheet_")
```

#### Optional: Filter for Subset of Runs
```{r}
sample.sheet <- sample.sheet %>% filter(seqrun == params$seqrun) %>% select(-seqrun)
```

#### Export & Transfer
>After exporting the sheet below, transfer the csv file to your working directory on the HCC.

```{r}
write.csv(sample.sheet, 
          paste0(params$local, "dataframes/dorado_sample_sheet_", params$seqrun, ".csv"),
          quote     = F,
          row.names = F)
```


```{r}
backup.df(sample.sheet, "dorado_sample_sheet")
sample.sheet %>%
  kbl(caption = paste0("Dorado sample sheet for sequencing run ", params$seqrun)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), fixed_thead = T, full_width = F) %>%
  scroll_box(width = "800px", height = "800px")
```


# Preprocessing

## Basecalling with Dorado  {.tabset}

>Technically we are supposed to use --constraint='gpu_v100|gpu_t4' to ensure the proper NVIDA gpu cores are assigned, but this seems to make the job request hang in waiting forever. So far I haven't run into issues without specifying, so I think most of the time we end up on the proper core. If your job fails for mysterious reasons, then try playing with this option though.
>Note that this constraint does seem to work when the job is subitted as a batch script, so you may want to go for that option with this task.

### Script Name

```{swan}
script="/work/richlab/aliciarich/microbiomes_loris/scripts/basecall_${params$seqrun}.sh"
```

### Interactive Job

```{swan open dorado gpu}
srun --partition=gpu,guest_gpu --time=2:30:00 --nodes=1 --ntasks=1 --cpus-per-task=8 --mem=256GB --gres=gpu:1 --job-name=basecall --pty $SHELL

cd "/work/richlab/aliciarich/microbiomes_loris/scripts/"
```

### Code

```{swan basecall dorado}
module purge
module load dorado-gpu/0.7

pod5="/work/richlab/aliciarich/ont_reads/loris_microbiome/hdz_raw/${params$seqrun}/"
basecalled="/work/richlab/aliciarich/ont_reads/loris_microbiome/basecalled/${params$seqrun}/"
mkdir -p $basecalled

dorado basecaller dna_r10.4.1_e8.2_400bps_sup@v5.0.0 \
    "$pod5" \
    --recursive \
    --no-trim \
    > "$basecalled/${params$seqrun}.bam" && \
dorado summary "$basecalled/${params$seqrun}.bam" > "$basecalled/${params$seqrun}_basecall_summary.tsv" 
```

## Demultiplex & Trim  {.tabset}

### Script Name

```{swan}
script="/work/richlab/aliciarich/microbiomes_loris/scripts/demux_${params$seqrun}.sh"
```

### Interactive Job

```{r}
open.job("demux", "100", "1", "1")
```


### Code

```{swan demux}
module purge
module load dorado/0.7

basecalled="/work/richlab/aliciarich/ont_reads/loris_microbiome/basecalled/${params$seqrun}/${params$seqrun}.bam"
sample_sheet="/work/richlab/aliciarich/microbiomes_loris/dataframes/dorado_sample_sheet_${params$seqrun}.csv"
trimmed="/work/richlab/aliciarich/ont_reads/loris_microbiome/trimmed/${params$seqrun}"

mkdir -p $trimmed

dorado demux "$basecalled" \
    --output-dir "$trimmed" \
    --kit-name "${params$kit_name}" \
    --sample-sheet "$sample_sheet" \
    --emit-fastq --emit-summary 
```

## Filter   {.tabset}
*Now we will use a loop with chopper to process each individual fastq file from the $trimmed directory and place a file with the same name into a directory for filtered reads.*  

### Open Interactive Job

```{swan filter job open}
srun --time=2:00:00 --job-name=filter --partition=guest --nodes=1 --ntasks-per-node=1 --mem=32GB --pty $SHELL
```

### Option A: Carry Out for All Sequencing Runs

#### Script Name

```{swan}
script="/work/richlab/aliciarich/microbiomes_loris/scripts/demux_${params$seqrun}filter_all.sh"
```


### Option B: Focus on a Single Sequencing Run

#### Script Name

```{swan}
script="/work/richlab/aliciarich/microbiomes_loris/scripts/demux_${params$seqrun}_filter.sh"
```


#### Code

##### Part 1: Run Job

```{swan activate filter}
module purge
module load anaconda

conda activate filter

trimmed="/work/richlab/aliciarich/ont_reads/loris_microbiome/trimmed/${params$seqrun}"
filtered="/work/richlab/aliciarich/ont_reads/loris_microbiome/filtered/${params$seqrun}"

mkdir -p $filtered

cd $trimmed


for file in "$trimmed"/*.fastq; do
    
    if [ -f "$file" ]; then
       
        base_filename=$(basename "$file")

        chopper --maxlength ${params$max_length} --minlength ${params$min_length} --quality ${params$min_qual} --input "$file" > "$filtered/$base_filename"

        echo "Processed $file"
    else
        echo "Error: File $file does not exist or is not a regular file."
    fi
done
```

```{swan}
conda deactivate
module purge
```


##### Part 2: Reorganize Directory Structure  
>The EPI2ME Workflows only recognize multiple files for input as individual samples if they are located in their own subdirectories, so the loop below will automatically create subdirectories within $filtered named for each sample_id and place the fastq file inside that directory.

```{swan filtered directory}
filtered="/work/richlab/aliciarich/ont_reads/loris_microbiome/filtered/${params$seqrun}"
cd $filtered

for file in "$filtered"/*.fastq; do
    if [ -f "$file" ]; then
        base_filename=$(basename "$file" .fastq)
        
        mkdir -p "$filtered/$base_filename"
        
        mv "$file" "$filtered/$base_filename/${base_filename}.fastq"
        
        echo "Organized $file"
    else
        echo "Error: File $file does not exist or is not a regular file."
    fi
done

```



# Processing  

## EPI2ME Nextflow wf-16s  {.tabset}

### Script Name

```{swan}
script="/work/richlab/aliciarich/microbiomes_loris/scripts/demux_${params$seqrun}_wf16s.sh"
```

### Interactive Job

```{r open nextflow job}
open.job("minimap", "350", "3", "32")
```

### Code
*Implementing the minimap2 verison of classification*
```{swan wf16s}
filtered="/work/richlab/aliciarich/ont_reads/loris_microbiome/filtered/${params$seqrun}"
processed="/work/richlab/aliciarich/microbiomes_loris/data/outputs_wf16s/${params$seqrun}"

module load "nextflow"

nextflow run epi2me-labs/wf-16s \
-profile singularity \
--fastq "$filtered" \
--taxonomic_rank "S" \
--keep_bam \
--minimap2_by_reference \
--out_dir "$processed" \
--min_len ${params$min_length} \
--max_len ${params$max_length} \
--abundance_threshold 0 \
--min_read_qual ${params$min_qual} \
--min_percent_identity ${params$min_id} \
--min_ref_coverage ${params$min_cov} \
--n_taxa_barplot ${params$ntaxa} \
--threads 32
```

# Conclusion

>*Load the output results to your local working R directory and continue to the Post Processing Script:*
