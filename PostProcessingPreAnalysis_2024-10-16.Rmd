---
title: "16S Pre-Analysis Post-Processing"
author: "Alicia Rich"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    css: "microbiome.css"
    toc: true
    toc_location: "before"
    toc_depth: 4
    number_sections: false
    toc_float: true
    code_folding: "hide"
    fig_caption: true
    bibliography: ["/Users/aliciamrich/RStudioMacbook/Bibliographies/loris_mb.bib"]
  link_citations: true
  csl: ["/Users/aliciamrich/RStudioMacbook/Bibliographies/plos.csl"]
params:
  seqrun:   "hdz17"
  date: !r Sys.Date()
  day1:     "2023-10-25"
  min_length: 1000
  max_length: 2000
  min_qual: 7
  min_id: 85
  min_cov: 80
  ntaxa: 12
  kit_name: "SQK-16S114-24"
  model: "dna_r10.4.1_e8.2_400bps_sup@v5.0.0"
  work_dir: "/work/richlab/aliciarich/microbiomes_loris/"
  local: "/Users/aliciamrich/RStudioMacbook/GitRepos/microbiomes_loris/"
  laptop: "/Users/aliciamrich/RStudioMacbook/GitRepos/microbiomes_loris/"
  desktop: "/Users/arich/Library/CloudStorage/GoogleDrive-aliciamrich@gmail.com/Other computers/My MacBook Pro/RStudioMacbook/GitRepos/microbiomes_loris/"
  kegg: "/Users/aliciamrich/Desktop/kegg/"
  downloads: "/Users/arich/Downloads"
  taxonomy.ordered: [superkingdom,
                     phylum,
                     class,
                     order,
                     family,
                     genus,
                     species]

editor_options: 
  chunk_output_type: inline
  
  
---

<style>
.swan-chunk {
  border: 1px solid #ccc;
  padding: 10px;
  margin: 10px 0;
  background-color: #f9f9f9;
  font-family: monospace;
  white-space: pre;
}
</style>


# Prepare Script

```{r message=FALSE, warning=FALSE, include=FALSE}
source(paste0(params$local, "dependencies/R/setup.R"))
source(paste0(params$local, "dependencies/R/microeco_setup.R"))
```

## Files Needed
<ol>
  <li><em>samples_metadata_*.csv</em></li>
  <li><em>*_abundance_table_species.tsv</em></li>
  <li><em>hdz_all_samples_*.csv</em></li>
  <li><em>*_wf-metagenomics-alignment.csv</em></li>
    <ul>
      <li><em>note that you must download each individual alignment file from the wf-16s html output and then use the terminal code to rename and relocate those for this part of the script to work</em></li>
    </ul>
  <li>abund_refseqs_aligned.fasta</li>
</ol>


## Note about Packages

>This workflow uses the MicroEco package, which incorporates several dependencies, and some of them can be a bit tricky to load and activate the first time around. I created a separate markdown file to walk you through the packages you will need. If this is your first time on this workflow, I recommend you start with that, ensure all packages have been installed, and then proceed with this.  

>For more information about MicroEco, see their [bookdown tutorial linked here.](https://chiliubio.github.io/microeco_tutorial/)

# Read In Sample Info

## Samples

> Load the master file created from the metadata workflow script.

```{r define metadata factors}
diet.levels        <-  c("baseline", "probiotic", "probiotic_oat", "steroid", "steroid_oat", "steroid_probiotic_oat")
diet.labels        <-  c("Base",     "PB",        "PB+O",          "S",       "S+O",         "S+PB+O")

subj.levels        <-  c("warble", "culi")
subj.labels        <-  c("W",      "C")

loc.levels        <-  c("old_enclosure", "new_enclosure")
loc.labels        <-  c("old",           "new")

estrus.levels     <-  c("estrus",      "not_estrus")
estrus.labels     <-  c("Y",           "N")

preg.levels     <-  c("pregnant",      "not_pregnant")
preg.labels     <-  c("Y",           "N")


access.levels     <-  c("together",    "separated")
access.labels     <-  c("Y",           "N")


samples   <- read.recent.version.tsv("dataframes", "samples_metadata_") %>%
                mutate(collection = ymd(collection)) %>%
                filter(!is.na(collection)) %>%
                mutate(subject    = factor(subject,      subj.levels,    labels = subj.labels),
                       diet       = factor(diet_trial,   diet.levels,    labels = diet.labels, ordered = T),
                       location   = factor(location,     loc.levels,     labels = loc.labels,  ordered = T),
                       estrus     = factor(warble_cycle, estrus.levels,  labels = estrus.labels),
                       pregnant   = factor(warble_cycle, preg.levels,     labels = preg.labels),
                       together   = factor(access,       access.levels,  labels = access.labels),
                       study_day  = since.start(collection, "days"),
                       study_week = ceiling(since.start(collection, "days")/7)) %>%
                mutate(subj_day   = str_glue("{subject}", "{study_day}"),
                       subj_diet  = factor(interaction(subject, diet))) %>%
                arrange(study_day, subject) %>%
                mutate(subj_day   = factor(subj_day, ordered = T)) %>%
                select(sampleID,
                       seqrun,
                       alias, 
                       subject,
                       collection,
                       subj_day,
                       study_day,
                       study_week,
                       diet,
                       location,
                       estrus,
                       pregnant,
                       together,
                       steroid_dose = ster_mgpday,
                       subj_diet,
                       bristol_min,
                       bristol_max,
                       bristol_mean
                       ) %>%
                arrange(subject, study_day)

samples.list <- samples %>% select(sampleID) %>% distinct()
```

## List of Aliases

### Transfer abundance table.

I keep a subdirectory within "data" called "outputs_wf16s". 
-   This is where all your nextflow output files should be deposited, each as a subdirectory for the individual sequencing run. 
-   Transfer the subdirectory for the sequencing run(s) you just finished processing from the Swan drive to your local matching directory. 
  -   I recommend renaming each file with the name for that sequencing run too (e.g., wf-16s.html becomes hdz17_wf-16s.html).
\
**Transfer abundance_table_species.tsv (now hdz17_abundance_table_species.tsv) into the parent directory "data".**

```{r}
abundance.files <- list.files(path = paste0(params$local, "data/"),
  pattern = "*_abundance_table_species.tsv$", full.names = TRUE)

abundance <- tibble()

if (length(abundance.files) > 0) {
  abundance <- read.tables(abundance.files[[1]]) %>%
               dplyr::select(-c(starts_with("total"))) %>%
               rename_with(~gsub(".", "-", .x, fixed = TRUE)) %>%
               fix.strings() 
  
  if (length(abundance.files) > 1) {
    for (i in 2:length(abundance.files)) {
      temp_data <- read.tables(abundance.files[[i]]) %>%
                   dplyr::select(-c(starts_with("total")))
      
      abundance <- full_join(abundance, temp_data) %>%
                   rename_with(~gsub(".", "-", .x, fixed = TRUE)) %>%
                   fix.strings() 
    }
  }
} else {
  stop("No abundance files found")
}

```


```{r}
filenames <- abundance %>% select(c(starts_with("hdz"), 
                                    starts_with("pl"))) %>%
              pivot_longer(cols = c(starts_with("hdz"),
                                    starts_with("pl")),
                                             names_to  = "alias",
                                             values_to = "abundance") %>%
                            select(alias) %>% distinct() %>% 
                           arrange(alias) %>%
                         mutate(file_append = (row_number() - 1)) %>%
                         mutate(old_name    = if_else(file_append == 0, "wf-metagenomics-alignment.csv",
                                                               str_glue("wf-metagenomics-alignment (", "{file_append}", ").csv")),
                                new_name    =      str_glue("{alias}", "_wf-metagenomics-alignment.csv"), 
                                .keep = "none")
export.list(filenames, "sampleid_files")
```

## Optional: If Bringing in a Single Sequencing Run

```{r eval=FALSE}
filenames <- read.recent.version.tsv("dataframes", "hdz_all_samples_") %>%
                select(alias, seqrun) %>% 
                filter(seqrun == paste0(params$seqrun)) %>%
                select(alias) %>% distinct() %>% arrange(alias) %>%
                mutate(file_append = (row_number() - 1)) %>%
                mutate(old_name    = if_else(file_append == 0, "wf-metagenomics-alignment.csv",
                                                      str_glue("wf-metagenomics-alignment (", "{file_append}", ").csv")),
                                new_name    = str_glue("{alias}", "_wf-metagenomics-alignment.csv"), 
                                .keep = "none")

export.list(filenames, "sampleid_files")
```

# Rename & Read In Alignment Tables from Downloads Folder

This step is pretty annoying, but until the developers update this gap I use the following workaround to import the raw alignment data for each sample and process it directly here in R.

## Download all Individual Alignment Tables

1.    Empty your local downloads folder so the directory is clean for the next steps.
2.    Open the html file ending in *wf-16s-report.html* in your browser. 
3.    Scroll down to the **Alignment Statistics** section. 
  -   Click *Export CSV*. Then use the dropdown box to select the next sample and repeat the *Export CSV* process. 
  -   Keep doing this for all 24 samples from your multiplexed run.
  -   This part is important: **You must be sure to download each table sequentially in the order it appears in the dropdown.**
    -   The code that I wrote below uses an ordered list of those sample IDs to rename the files in your downloads folder based on the order in which you downloaded them (e.g., *wf-metagenomics-alignment (2).csv* and *wf-metagenomics-alignment (3).csv* may become *hdz-489-s391_wf-metagenomics-alignment.csv* and *hdz-488-s390_wf-metagenomics-alignment.csv*).
4.    **Only after you have downloaded all 24 csv files into your Downloads directory:** Run the code below from the Terminal tab once you have navigated to your local Downloads directory. 
  -   It will simultaneously rename all 24 files and transfer them to the local working subdirectory "*data*".
  -   The rest of the script below uses this filename system to import and merge the files, matching every aligned read to its sampleID.
  -   If it worked, then all the files should disappear from your Downloads directory and you should see 24 new csv files in your data directory ending in *wf-metagenomics-alignment.csv*

## Use Local Terminal to rename and Transfer Files:

```{swan}
filenames="${params$local}dataframes/sampleid_files.txt"
target_dir="${params$local}data"
downloads="${params$downloads}"

cd $downloads

while IFS=$'\t' read -r old_name new_name; do
    mv "$old_name" "$target_dir/$new_name"
done < "$filenames"
```

# Read & Coalate All Alignment Files

```{r read in alignments, warning=F}
alignment.files.list <- list.files(path = paste0(params$local, "data"),
  pattern = "*_wf-metagenomics-alignment.csv$", full.names = TRUE)
alignment.files      <- lapply(alignment.files.list, read_alignment_file)
alias                <- str_extract(alignment.files.list, "hdz-.+(?=_wf-metagenomics-alignment.csv$)")

alignment.files <- Map(function(df, id) {
  if (nrow(df) > 0) {
    df$alias <- id
  } else {
    warning(paste("Data frame for", id, "is empty. Alias not assigned."))
  }
  return(df)
}, alignment.files, alias)

alignments.long        <- bind_rows(alignment.files) %>%
                       as_tibble() %>% fix.strings()  %>% 
                        select(ref,
                               taxid,
                               species,
                               genus,
                               family,
                               order,
                               class,
                               phylum,
                               superkingdom,
                               alias,
                               coverage,
                               n_reads = number.of.reads) %>%
                        filter(coverage >= 80 & !is.na(alias)) %>%
                        mutate(sampleID = str_sub(alias, 1L, 7L)) %>%
                        semi_join(samples.list, by=join_by(sampleID)) %>%
                        left_join(select(samples, sampleID, subj_day), by = join_by(sampleID)) %>%
                        group_by(ref,
                                 taxid,
                                 species,
                                 genus,
                                 family,
                                 order,
                                 class,
                                 phylum,
                                 superkingdom,
                                 subj_day) %>%
                        summarize(samp_cov     = mean(coverage),
                                  samp_n_reads = mean(n_reads)) %>% ungroup()

alignments.refs <- alignments.long %>% 
                        group_by(ref,
                                 taxid,
                                 species,
                                 genus,
                                 family,
                                 order,
                                 class,
                                 phylum,
                                 superkingdom) %>%
                        summarize(ref_n_reads   = round(sum(samp_n_reads),  digits = 2),
                                  ref_mean_cov  = round(mean(samp_cov),     digits = 2)) %>%
                        ungroup() %>% group_by(species,
                                               genus,
                                               family,
                                               order,
                                               class,
                                               phylum,
                                               superkingdom) %>%
                        arrange(species, desc(ref_n_reads), desc(ref_mean_cov)) %>%
                        mutate(ref_order       = row_number(),
                               tax_total_count = sum(ref_n_reads)) %>%
                        ungroup() %>%
                        filter(ref_order == 1) %>%
                        select(-ref_order) %>% sort.taxa()

alignments    <- alignments.long %>% 
                        group_by(superkingdom,
                                 phylum,
                                 class,
                                 order,
                                 family,
                                 genus,
                                 species,
                                 subj_day) %>%
                          summarize(counts = sum(samp_n_reads)) %>%
                          ungroup() %>%
                          left_join(alignments.refs,
                    by = join_by(superkingdom,
                                 phylum,
                                 class,
                                 order,
                                 family,
                                 genus,
                                 species)) %>%
                    pivot_wider(id_cols    = c(ref,
                                               taxid,
                                               params$taxonomy.ordered,
                                               ref_mean_cov),
                                names_from  = subj_day,
                                values_from = counts) %>%
                    sort.taxa() %>%
                    mutate(across(where(is.numeric), ~replace_na(.x, 0)),
                           organism = str_glue("txid", "{taxid}")) 
```

# Update Reference Tree & FASTA

>ONT Reads are generally still too messy and long to smoothly produce consensus sequences for each taxon, so instead, we will use Entrez Direct to source a reference fasta for each of the references that minimap2 already matched to our reads.

## Check Previous Taxonomy List

```{r eval=FALSE}
rep.seqs.previous <- read.fasta(paste0(params$local, "refs/abund_refseqs_aligned.fasta"))

rep.seqs.txids    <- tibble(getName(rep.seqs.previous)) %>% 
  mutate(organism  =        getName(rep.seqs.previous)) %>% 
  mutate(taxid     = as.numeric(str_remove_all(organism, "[^\\d]"))) %>%
  select(taxid)

new.refs          <- alignments %>% anti_join(rep.seqs.txids) %>% 
                     select(ref, organism) %>% distinct()

export.list(new.refs, "fetch_references")
```

>Transfer the fetch_references.txt file from the local dataframes directory to the matching directory on Swan and then run the script below.

## Fetch New References Using Entrez Direct

### Job Options {.tabset}

#### Interactive Job

```{r}
open.job("fetch", "200", "1", "1")
```

#### Batch Header

```{swan}
#!/bin/bash
#SBATCH --time=1:00:00
#SBATCH --job-name=fetch_refs
#SBATCH --error=${params$work_dir}logs/fetch_refs.%J.err
#SBATCH --output=${params$work_dir}logs/fetch_refs.%J.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=200GB
#SBATCH --partition=guest

module purge
module load entrez-direct
module load seqkit 

cd ${params$work_dir}refs

uid_file="${params$work_dir}dataframes/fetch_references.txt"
accessions_file="${params$work_dir}refs/accessions.txt"
fasta_file="abund_refseqs_new.fasta"
renamed_fasta="abund_refseqs_new_renamed.fasta"


```

#### Code

```{r}
load.pkg("entrez-direct", "")
load.pkg("seqkit", "")
```

```{swan}

> "$fasta_file"

cut -f1 "$uid_file" > "$accessions_file"

if efetch -db nuccore -input "$accessions_file" -format fasta -email "aliciarich@unomaha.edu" > "$fasta_file"; then
    echo "Sequences fetched successfully."
else
    echo "Error fetching sequences." >&2
    exit 1
fi

seqkit replace $fasta_file -p '^(\S+)(.+?)$' -r '{kv}$2' -k $uid_file -o $renamed_fasta

rm $fasta_file
mv $renamed_fasta $fasta_file
```

>Once the script finishes running, transfer the abund_refseqs_new.fasta file from the "refs" directory on Swan to the local matching directory to run the next step in R.


## Join New References to Old FASTA

```{r, eval=F}
new.refseqs <- read.fasta(paste0(params$local, "refs/abund_refseqs_new.fasta"))

merged.refseqs <- c(new.refseqs, rep.seqs.previous)

write.fasta(sequences =       merged.refseqs,
            names     = names(merged.refseqs),
            file.out  = paste0(params$local, "refs/refseqs_toalign.fasta"))
```

>Transfer this fasta file from your local "refs" directory over to the matching directory on Swan and then run the script below.

## Realign Sequences and Assemble Updated Tree

### Job Options {.tabset}

#### Interactive Job

```{r}
open.job("tree", "250", "3", "1")
```

#### Batch Header

```{swan}
#!/bin/bash
#SBATCH --time=3:00:00
#SBATCH --job-name=align_tree
#SBATCH --error=${params$work_dir}logs/align_tree.%J.err
#SBATCH --output=${params$work_dir}logs/align_tree.%J.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=250GB
#SBATCH --partition=guest

module purge
module load mafft 
module load fasttree 

cd ${params$work_dir}refs

```

#### Code

```{r}
load.pkg("mafft", "")
load.pkg("fasttree", "")
```

```{swan}
input_fasta="refseqs_toalign.fasta"
aligned_fasta="abund_refseqs_aligned.fasta"
tree_file="repseqs_tree.newick"

mafft --auto $input_fasta > $aligned_fasta

FastTree -nt $aligned_fasta > $tree_file
```

>Wait for the script to finish running and then transfer all updated files from the microbiomes_loris/refs directory on Swan to your local matching directory.

# Prepare Dataset for Analysis

## Read in New Phylogenetic Tree & Representative FASTA

```{r}
tax.tree <- read.tree(paste0(params$local, "refs/repseqs_tree.newick"))
rep.seqs <- read.fasta(paste0(params$local, "refs/abund_refseqs_aligned.fasta"))
rep.seqs.tax4fun <- lapply(rep.seqs, function(seq) {gsub("-", "", seq)})
```


## Taxonomy Table

```{r}
tax.table <- alignments %>% 
                            column_to_rownames("organism") %>%
                            select(params$taxonomy.ordered) %>% 
                            sort.taxa() %>%
                            rename_with(~str_to_title(.x), .cols = everything()) %>%
                            mutate(Kingdom = Superkingdom, .keep = "unused") %>%
                            relocate(Kingdom) %>%
                            distinct() %>%
                tidy_taxonomy()
```

### Back Up Taxonomy Table

```{r}
taxonomy.list <- alignments %>% 
                            select(organism, params$taxonomy.ordered) %>% 
                            sort.taxa() %>%
                            rename_with(~str_to_title(.x), .cols = everything()) %>%
                            mutate(Kingdom = Superkingdom, .keep = "unused") %>%
                            relocate(Organism, Kingdom)

backup.df(taxonomy.list, "taxonomy_list")
```

## OTU Table

```{r}
otu.table <- alignments %>% select(organism,
                             matches("^\\w\\d+$")) %>%
                      column_to_rownames("organism")

otu.samples <- otu.table %>% pivot_longer(matches("^\\w\\d+$"),
                                          names_to  = "subj_day",
                                          values_to = "counts") %>%
                            select(subj_day) %>% arrange(subj_day) %>% distinct()
```

## Sample Table

> That includes collapsing samples with technical replicates into single rows and filtering to ensure only samples from OTU table are kept.
> I will also create interaction variables to help with nested grouping and comparisons later.
> And I will include numeric dummy variables for all factors to work with modeling.

                            

```{r}
sample.table <- samples  %>% select(-c(alias, sampleID)) %>%
                            distinct(subj_day, .keep_all = T) %>%
                            semi_join(otu.samples, by = join_by(subj_day)) %>%
                            arrange(study_day, subject) %>%
                            column_to_rownames("subj_day") %>%
                            mutate(subj_loc     = factor(interaction(subject, location)),
                                   subj_week    = factor(interaction(subject, study_week)),
                                   subj_togeth  = factor(interaction(subject, together)),
                                   diet_steroid = factor(interaction(diet,    steroid_dose)),
                                   diet_week    = factor(interaction(diet,    study_week))) %>%
                            mutate(diet.num = case_when(diet     == "Base"    ~ 1,
                                                        diet     == "PB"      ~ 2,
                                                        diet     == "PB+O"    ~ 3,
                                                        diet     == "S"       ~ 4,
                                                        diet     == "S+O"     ~ 5,
                                                        diet     == "S+PB+O"  ~ 6),
                                   subj.num = case_when(subject  == "W"       ~ 1,
                                                        subject  == "C"       ~ 2),
                                   loc.num  = case_when(location == "old"     ~ 1,
                                                        location == "new"     ~ 2),
                                   est.num  = case_when(estrus   == "Y"       ~ 1,
                                                        estrus   == "N"       ~ 2),
                                   pre.num  = case_when(pregnant == "Y"       ~ 1,
                                                        pregnant == "N"       ~ 2),
                                   acc.num  = case_when(together == "Y"       ~ 1,
                                                        together == "N"       ~ 2))
```

### Subset Versions

```{r}
otu.culi     <-  otu.table    %>% select(starts_with("C"))
samples.culi <-  sample.table %>% filter(subject  == "C") %>%
                  select(-c(starts_with("subj_")))
otu.warb     <-  otu.table    %>% select(starts_with("W"))
samples.warb <-  sample.table %>% filter(subject  == "W") %>%
                  select(-c(starts_with("subj_"),
                            starts_with("bristol")))

samp.order.main <- factor(rownames(sample.table), ordered = T)
samp.order.culi <- factor(rownames(samples.culi), ordered = T)
samp.order.warb <- factor(rownames(samples.warb), ordered = T)
```


# MicroEco Dataset

>Load subsetted sample tables to create dataset objects and then rarefy all datasets to read count of 7,000.  
>  *We may update this in the future with a larger sample size to a count as high as 10,000*

```{r}
dataset.main  <- microtable$new(
                               sample_table = sample.table,
                               otu_table    = otu.table,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs,
                               auto_tidy    = T)
dataset.culi <- microtable$new(
                               sample_table = samples.culi,
                               otu_table    = otu.culi,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs,
                               auto_tidy    = T)
dataset.warb <- microtable$new(
                               sample_table = samples.warb,
                               otu_table    = otu.warb,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs,
                               auto_tidy    = T)

dataset.main$rarefy_samples(method = "SRS", sample.size = 7000)
dataset.culi$rarefy_samples(method = "SRS", sample.size = 7000)
dataset.warb$rarefy_samples(method = "SRS", sample.size = 7000)
```

## Basic Stats

### Calculate Relative Abundances

>This will replace our raw read counts with relative abundance values and store them in our OTU table. Then we will be able to do additional data cleaning based on relative abundances.

```{r}
dataset.main$cal_abund()
dataset.culi$cal_abund()
dataset.warb$cal_abund()
```

### Filter Low Abundance Taxa

>Now we want to remove all taxa with low abundances (0.01%) so that they are not giving us muddy results.
>We are also removing taxa that appear in less than 1% of all samples.
>>*Updating this to 2% as of 10-16-2024*

```{r}
dataset.main$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)
dataset.culi$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)
dataset.warb$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)
```

### Calculate Diversity Metrics

>Now that we have our cleaned datasets, we can add alpha and beta diversity measures to the objects.

```{r}
dataset.main$cal_alphadiv()
dataset.culi$cal_alphadiv()
dataset.warb$cal_alphadiv()
dataset.main$cal_betadiv(unifrac = T, method = "canberra")
dataset.culi$cal_betadiv(unifrac = T, method = "canberra")
dataset.warb$cal_betadiv(unifrac = T, method = "canberra")
```

## Clone and Merge Datasets at Different Taxonomic Levels

```{r}
gen.data.main <- clone(dataset.main, deep = T)
gen.data.culi <- clone(dataset.culi, deep = T)
gen.data.warb <- clone(dataset.warb, deep = T)

gen.data.main$merge_taxa(taxa = "Genus")
gen.data.culi$merge_taxa(taxa = "Genus")
gen.data.warb$merge_taxa(taxa = "Genus")

fam.data.main <- clone(dataset.main, deep = T)
fam.data.culi <- clone(dataset.culi, deep = T)
fam.data.warb <- clone(dataset.warb, deep = T)

fam.data.main$merge_taxa(taxa = "Family")
fam.data.culi$merge_taxa(taxa = "Family")
fam.data.warb$merge_taxa(taxa = "Family")

ord.data.main <- clone(dataset.main, deep = T)
ord.data.culi <- clone(dataset.culi, deep = T)
ord.data.warb <- clone(dataset.warb, deep = T)

ord.data.main$merge_taxa(taxa = "Order")
ord.data.culi$merge_taxa(taxa = "Order")
ord.data.warb$merge_taxa(taxa = "Order")

cla.data.main <- clone(dataset.main, deep = T)
cla.data.culi <- clone(dataset.culi, deep = T)
cla.data.warb <- clone(dataset.warb, deep = T)

cla.data.main$merge_taxa(taxa = "Class")
cla.data.culi$merge_taxa(taxa = "Class")
cla.data.warb$merge_taxa(taxa = "Class")

phy.data.main <- clone(dataset.main, deep = T)
phy.data.culi <- clone(dataset.culi, deep = T)
phy.data.warb <- clone(dataset.warb, deep = T)

phy.data.main$merge_taxa(taxa = "Phylum")
phy.data.culi$merge_taxa(taxa = "Phylum")
phy.data.warb$merge_taxa(taxa = "Phylum")
```

# Export Datasets for Further Analysis

```{r echo=FALSE, warning=FALSE, message=FALSE}
 dataset.main$save_table(paste0(params$local, "dataframes/spe_dataset_main"), sep = "\t")
 dataset.culi$save_table(paste0(params$local, "dataframes/spe_dataset_culi"), sep = "\t")
 dataset.warb$save_table(paste0(params$local, "dataframes/spe_dataset_warb"), sep = "\t")
gen.data.main$save_table(paste0(params$local, "dataframes/gen_dataset_main"), sep = "\t")
gen.data.culi$save_table(paste0(params$local, "dataframes/gen_dataset_culi"), sep = "\t")
gen.data.warb$save_table(paste0(params$local, "dataframes/gen_dataset_warb"), sep = "\t")
fam.data.main$save_table(paste0(params$local, "dataframes/fam_dataset_main"), sep = "\t")
fam.data.culi$save_table(paste0(params$local, "dataframes/fam_dataset_culi"), sep = "\t")
fam.data.warb$save_table(paste0(params$local, "dataframes/fam_dataset_warb"), sep = "\t")
ord.data.main$save_table(paste0(params$local, "dataframes/ord_dataset_main"), sep = "\t")
ord.data.culi$save_table(paste0(params$local, "dataframes/ord_dataset_culi"), sep = "\t")
ord.data.warb$save_table(paste0(params$local, "dataframes/ord_dataset_warb"), sep = "\t")
cla.data.main$save_table(paste0(params$local, "dataframes/cla_dataset_main"), sep = "\t")
cla.data.culi$save_table(paste0(params$local, "dataframes/cla_dataset_culi"), sep = "\t")
cla.data.warb$save_table(paste0(params$local, "dataframes/cla_dataset_warb"), sep = "\t")
phy.data.main$save_table(paste0(params$local, "dataframes/phy_dataset_main"), sep = "\t")
phy.data.culi$save_table(paste0(params$local, "dataframes/phy_dataset_culi"), sep = "\t")
phy.data.warb$save_table(paste0(params$local, "dataframes/phy_dataset_warb"), sep = "\t")
```

## Export Tables with Basic Stats

```{r message=FALSE}
 dataset.main$save_abund(paste0(params$local, "dataframes/spe_abund_main"), sep = "\t")
 dataset.culi$save_abund(paste0(params$local, "dataframes/spe_abund_culi"), sep = "\t")
 dataset.warb$save_abund(paste0(params$local, "dataframes/spe_abund_warb"), sep = "\t")
gen.data.main$save_abund(paste0(params$local, "dataframes/gen_abund_main"), sep = "\t")
gen.data.culi$save_abund(paste0(params$local, "dataframes/gen_abund_culi"), sep = "\t")
gen.data.warb$save_abund(paste0(params$local, "dataframes/gen_abund_warb"), sep = "\t")
fam.data.main$save_abund(paste0(params$local, "dataframes/fam_abund_main"), sep = "\t")
fam.data.culi$save_abund(paste0(params$local, "dataframes/fam_abund_culi"), sep = "\t")
fam.data.warb$save_abund(paste0(params$local, "dataframes/fam_abund_warb"), sep = "\t")
ord.data.main$save_abund(paste0(params$local, "dataframes/ord_abund_main"), sep = "\t")
ord.data.culi$save_abund(paste0(params$local, "dataframes/ord_abund_culi"), sep = "\t")
ord.data.warb$save_abund(paste0(params$local, "dataframes/ord_abund_warb"), sep = "\t")
cla.data.main$save_abund(paste0(params$local, "dataframes/cla_abund_main"), sep = "\t")
cla.data.culi$save_abund(paste0(params$local, "dataframes/cla_abund_culi"), sep = "\t")
cla.data.warb$save_abund(paste0(params$local, "dataframes/cla_abund_warb"), sep = "\t")
phy.data.main$save_abund(paste0(params$local, "dataframes/phy_abund_main"), sep = "\t")
phy.data.culi$save_abund(paste0(params$local, "dataframes/phy_abund_culi"), sep = "\t")
phy.data.warb$save_abund(paste0(params$local, "dataframes/phy_abund_warb"), sep = "\t")

 dataset.main$save_alphadiv(paste0(params$local, "dataframes/spe_alphadiv_main"))
 dataset.culi$save_alphadiv(paste0(params$local, "dataframes/spe_alphadiv_culi"))
 dataset.warb$save_alphadiv(paste0(params$local, "dataframes/spe_alphadiv_warb"))
gen.data.main$save_alphadiv(paste0(params$local, "dataframes/gen_alphadiv_main"))
gen.data.culi$save_alphadiv(paste0(params$local, "dataframes/gen_alphadiv_culi"))
gen.data.warb$save_alphadiv(paste0(params$local, "dataframes/gen_alphadiv_warb"))
fam.data.main$save_alphadiv(paste0(params$local, "dataframes/fam_alphadiv_main"))
fam.data.culi$save_alphadiv(paste0(params$local, "dataframes/fam_alphadiv_culi"))
fam.data.warb$save_alphadiv(paste0(params$local, "dataframes/fam_alphadiv_warb"))
ord.data.main$save_alphadiv(paste0(params$local, "dataframes/ord_alphadiv_main"))
ord.data.culi$save_alphadiv(paste0(params$local, "dataframes/ord_alphadiv_culi"))
ord.data.warb$save_alphadiv(paste0(params$local, "dataframes/ord_alphadiv_warb"))
cla.data.main$save_alphadiv(paste0(params$local, "dataframes/cla_alphadiv_main"))
cla.data.culi$save_alphadiv(paste0(params$local, "dataframes/cla_alphadiv_culi"))
cla.data.warb$save_alphadiv(paste0(params$local, "dataframes/cla_alphadiv_warb"))
phy.data.main$save_alphadiv(paste0(params$local, "dataframes/phy_alphadiv_main"))
phy.data.culi$save_alphadiv(paste0(params$local, "dataframes/phy_alphadiv_culi"))
phy.data.warb$save_alphadiv(paste0(params$local, "dataframes/phy_alphadiv_warb"))

 dataset.main$save_betadiv(paste0(params$local, "dataframes/spe_betadiv_main"))
 dataset.culi$save_betadiv(paste0(params$local, "dataframes/spe_betadiv_culi"))
 dataset.warb$save_betadiv(paste0(params$local, "dataframes/spe_betadiv_warb"))
gen.data.main$save_betadiv(paste0(params$local, "dataframes/gen_betadiv_main"))
gen.data.culi$save_betadiv(paste0(params$local, "dataframes/gen_betadiv_culi"))
gen.data.warb$save_betadiv(paste0(params$local, "dataframes/gen_betadiv_warb"))
fam.data.main$save_betadiv(paste0(params$local, "dataframes/fam_betadiv_main"))
fam.data.culi$save_betadiv(paste0(params$local, "dataframes/fam_betadiv_culi"))
fam.data.warb$save_betadiv(paste0(params$local, "dataframes/fam_betadiv_warb"))
ord.data.main$save_betadiv(paste0(params$local, "dataframes/ord_betadiv_main"))
ord.data.culi$save_betadiv(paste0(params$local, "dataframes/ord_betadiv_culi"))
ord.data.warb$save_betadiv(paste0(params$local, "dataframes/ord_betadiv_warb"))
cla.data.main$save_betadiv(paste0(params$local, "dataframes/cla_betadiv_main"))
cla.data.culi$save_betadiv(paste0(params$local, "dataframes/cla_betadiv_culi"))
cla.data.warb$save_betadiv(paste0(params$local, "dataframes/cla_betadiv_warb"))
phy.data.main$save_betadiv(paste0(params$local, "dataframes/phy_betadiv_main"))
phy.data.culi$save_betadiv(paste0(params$local, "dataframes/phy_betadiv_culi"))
phy.data.warb$save_betadiv(paste0(params$local, "dataframes/phy_betadiv_warb"))
```

## Add Functional Profiles 

### FAPROTAX

```{r message=FALSE}
fpt.main <- trans_func$new(dataset.main)
fpt.culi <- trans_func$new(dataset.culi)
fpt.warb <- trans_func$new(dataset.warb)

fpt.main$cal_spe_func(prok_database = "FAPROTAX")
fpt.culi$cal_spe_func(prok_database = "FAPROTAX")
fpt.warb$cal_spe_func(prok_database = "FAPROTAX")

fpt.main$cal_spe_func_perc(abundance_weighted = T)
fpt.culi$cal_spe_func_perc(abundance_weighted = T)
fpt.warb$cal_spe_func_perc(abundance_weighted = T)
```

#### Transpose Results & Add to Microtable Object for Analysis

```{r message=FALSE}
fpt.main$trans_spe_func_perc()
fpt.culi$trans_spe_func_perc()
fpt.warb$trans_spe_func_perc()

fpt.main.df <- as.data.frame(t(fpt.main$res_spe_func_perc), check.names = F)
fpt.culi.df <- as.data.frame(t(fpt.culi$res_spe_func_perc), check.names = F)
fpt.warb.df <- as.data.frame(t(fpt.warb$res_spe_func_perc), check.names = F)

fpt.main.tax <- as.data.frame(rownames(fpt.main.df), nm = c("Function")) %>% mutate(func_name = str_glue("fpt", "{row_number()}")) 
fpt.culi.tax <- as.data.frame(rownames(fpt.culi.df), nm = c("Function")) %>% mutate(func_name = str_glue("fpt", "{row_number()}")) 
fpt.warb.tax <- as.data.frame(rownames(fpt.warb.df), nm = c("Function")) %>% mutate(func_name = str_glue("fpt", "{row_number()}")) 

fpt.main.otu <- fpt.main.df %>% rownames_to_column("Function") %>% left_join(fpt.main.tax) %>% column_to_rownames("func_name") %>% select(-Function)
fpt.culi.otu <- fpt.culi.df %>% rownames_to_column("Function") %>% left_join(fpt.culi.tax) %>% column_to_rownames("func_name") %>% select(-Function)
fpt.warb.otu <- fpt.warb.df %>% rownames_to_column("Function") %>% left_join(fpt.warb.tax) %>% column_to_rownames("func_name") %>% select(-Function)

fpt.main.tax <- fpt.main.tax %>% column_to_rownames("func_name")
fpt.culi.tax <- fpt.culi.tax %>% column_to_rownames("func_name")
fpt.warb.tax <- fpt.warb.tax %>% column_to_rownames("func_name")
```


```{r message=FALSE}
fpt.data.main <- microtable$new(otu_table = fpt.main.otu, tax_table = fpt.main.tax, sample_table = dataset.main$sample_table)
fpt.data.culi <- microtable$new(otu_table = fpt.culi.otu, tax_table = fpt.culi.tax, sample_table = dataset.culi$sample_table)
fpt.data.warb <- microtable$new(otu_table = fpt.warb.otu, tax_table = fpt.warb.tax, sample_table = dataset.warb$sample_table)

fpt.data.main$tidy_dataset()
fpt.data.culi$tidy_dataset()
fpt.data.warb$tidy_dataset()

fpt.data.main$cal_abund()
fpt.data.culi$cal_abund()
fpt.data.warb$cal_abund()

fpt.data.main$cal_betadiv(method = "canberra")
fpt.data.culi$cal_betadiv(method = "canberra")
fpt.data.warb$cal_betadiv(method = "canberra")
```

#### Export Dataset for Further Analysis

```{r message=FALSE}
fpt.data.main$save_table(paste0(params$local, "dataframes/fpt_dataset_main"), sep = "\t")
fpt.data.culi$save_table(paste0(params$local, "dataframes/fpt_dataset_culi"), sep = "\t")
fpt.data.warb$save_table(paste0(params$local, "dataframes/fpt_dataset_warb"), sep = "\t")
```

##### Table Version of Basic Stats

```{r message=FALSE}
fpt.data.main$save_abund(  paste0(params$local, "dataframes/fpt_abund_main"), sep = "\t")
fpt.data.culi$save_abund(  paste0(params$local, "dataframes/fpt_abund_culi"), sep = "\t")
fpt.data.warb$save_abund(  paste0(params$local, "dataframes/fpt_abund_warb"), sep = "\t")
fpt.data.main$save_betadiv(paste0(params$local, "dataframes/fpt_betadiv_main"))
fpt.data.culi$save_betadiv(paste0(params$local, "dataframes/fpt_betadiv_culi"))
fpt.data.warb$save_betadiv(paste0(params$local, "dataframes/fpt_betadiv_warb"))
```


### NJC19

```{r message=FALSE}
njc.main <- trans_func$new(dataset.main)
njc.culi <- trans_func$new(dataset.culi)
njc.warb <- trans_func$new(dataset.warb)
njc.main$cal_spe_func(prok_database = "NJC19")
njc.culi$cal_spe_func(prok_database = "NJC19")
njc.warb$cal_spe_func(prok_database = "NJC19")
njc.main$cal_spe_func_perc(abundance_weighted = T)
njc.culi$cal_spe_func_perc(abundance_weighted = T)
njc.warb$cal_spe_func_perc(abundance_weighted = T)
```

#### Transpose Results & Add to Microtable Object for Analysis

```{r message=FALSE}
njc.main$trans_spe_func_perc()
njc.culi$trans_spe_func_perc()
njc.warb$trans_spe_func_perc()
njc.main.df  <-        as.data.frame(t(njc.main$res_spe_func_perc), check.names = F)
njc.culi.df  <-        as.data.frame(t(njc.culi$res_spe_func_perc), check.names = F)
njc.warb.df  <-        as.data.frame(t(njc.warb$res_spe_func_perc), check.names = F)
njc.main.tax <- as.data.frame(rownames(njc.main.df), nm = c("Function")) %>% mutate(func_name = str_glue("njc", "{row_number()}")) 
njc.culi.tax <- as.data.frame(rownames(njc.culi.df), nm = c("Function")) %>% mutate(func_name = str_glue("njc", "{row_number()}")) 
njc.warb.tax <- as.data.frame(rownames(njc.warb.df), nm = c("Function")) %>% mutate(func_name = str_glue("njc", "{row_number()}")) 
njc.main.otu <- njc.main.df %>% rownames_to_column("Function") %>% left_join(njc.main.tax) %>% column_to_rownames("func_name") %>% select(-Function)
njc.culi.otu <- njc.culi.df %>% rownames_to_column("Function") %>% left_join(njc.culi.tax) %>% column_to_rownames("func_name") %>% select(-Function)
njc.warb.otu <- njc.warb.df %>% rownames_to_column("Function") %>% left_join(njc.warb.tax) %>% column_to_rownames("func_name") %>% select(-Function)
njc.main.tax <- njc.main.tax %>% column_to_rownames("func_name")
njc.culi.tax <- njc.culi.tax %>% column_to_rownames("func_name")
njc.warb.tax <- njc.warb.tax %>% column_to_rownames("func_name")
```


```{r message=FALSE}
njc.data.main <- microtable$new(otu_table = njc.main.otu, tax_table = njc.main.tax, sample_table = dataset.main$sample_table)
njc.data.culi <- microtable$new(otu_table = njc.culi.otu, tax_table = njc.culi.tax, sample_table = dataset.culi$sample_table)
njc.data.warb <- microtable$new(otu_table = njc.warb.otu, tax_table = njc.warb.tax, sample_table = dataset.warb$sample_table)
njc.data.main$tidy_dataset()
njc.data.culi$tidy_dataset()
njc.data.warb$tidy_dataset()
njc.data.main$cal_abund()
njc.data.culi$cal_abund()
njc.data.warb$cal_abund()
njc.data.main$cal_betadiv(method = "canberra")
njc.data.culi$cal_betadiv(method = "canberra")
njc.data.warb$cal_betadiv(method = "canberra")
```

#### Export Dataset for Further Analysis

```{r message=FALSE}
njc.data.main$save_table(paste0(params$local, "dataframes/njc_dataset_main"), sep = "\t")
njc.data.culi$save_table(paste0(params$local, "dataframes/njc_dataset_culi"), sep = "\t")
njc.data.warb$save_table(paste0(params$local, "dataframes/njc_dataset_warb"), sep = "\t")
```

##### Table Version of Basic Stats

```{r message=FALSE}
njc.data.main$save_abund(  paste0(params$local, "dataframes/njc_abund_main"  ), sep = "\t")
njc.data.culi$save_abund(  paste0(params$local, "dataframes/njc_abund_culi"  ), sep = "\t")
njc.data.warb$save_abund(  paste0(params$local, "dataframes/njc_abund_warb"  ), sep = "\t")
njc.data.main$save_betadiv(paste0(params$local, "dataframes/njc_betadiv_main"))
njc.data.culi$save_betadiv(paste0(params$local, "dataframes/njc_betadiv_culi"))
njc.data.warb$save_betadiv(paste0(params$local, "dataframes/njc_betadiv_warb"))
```

### KEGG Pathways

>Note that getting Tax4Fun2 to work can be tricky at first. I recommend you create a local directory copy called "kegg" on your desktop with the NCBI Blast database and the Tax4Fun2 Reference database stored there and then update the paths below to reflect that. You should only have to set this up once the first time you use these tools on a given hard drive.

#### Download the Reference Database for Tax4Fun2 and Extract All Files

[Click here to download the database](https://zenodo.org/records/10035668/files/Tax4Fun2_ReferenceData_v2.tar.gz?download=1)

Then run the chunk below to extract all files.  
```{swan}
tar -xvzf Tax4Fun2_ReferenceData_v2.tar.gz
```

#### Download the Blast+ Tools by running this code from the target directory {.tabset}

##### For Mac Computers with the newer Silicon Chips:

```{swan}
curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-aarch64-macosx.tar.gz
tar -xvzf ncbi-blast-2.16.0+-aarch64-macosx.tar.gz
```

##### For Mac Computers with the older Intel systems:

```{swan}
curl -O ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/ncbi-blast-2.16.0+-x64-macosx.tar.gz
tar -xvzf ncbi-blast-2.16.0+-x64-macosx.tar.gz
```

#### Make Sure the BLAST+ Tools are Executable

Run the following chunk from the same directory where you have extracted your Blast Tools

```{swan}
chmod 755 ncbi-blast-2.16.0+/bin/*
  
chmod -R 755 Tax4Fun2_ReferenceData_v2/*
```

>If you are on a mac and still get an error about executing blastn after running the Tax4Fun2 commands, then you may still need to go to your security preferences and select "Open anyway" where it says something about blocking Blastn.

You can also run the chunk below from the R console to make sure R is able to read and write to the directory.

```{r eval=FALSE}
dir_path <- paste0(params$kegg, "Tax4Fun2_ReferenceData_v2/Ref99NR")
if (file.access(dir_path, mode = 4) == 0) {
  print("Directory is readable.")
} else {
  print("Directory is not accessible.")
}
```

```{r eval=FALSE}
dir_path <- paste0(params$kegg, "ncbi-blast-2.16.0+/bin")
if (file.access(dir_path, mode = 4) == 0) {
  print("Directory is readable.")
} else {
  print("Directory is not accessible.")
}
```


#### Create New Datasets

>The Tax4Fun2 functions can be very glitchy. One of the glitches I have found is that aligned fasta records with hyphens for gaps won't work against the NR99 reference database. So I recommend creating a version of that file without any of the gaps included and put it into a new dataset with the same parameters for rarefaction, etc.

```{r message=F}
dataset.keg.main  <- microtable$new(
                               sample_table = sample.table,
                               otu_table    = otu.table,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs.tax4fun,
                               auto_tidy    = T)
dataset.keg.culi <- microtable$new(
                               sample_table = samples.culi,
                               otu_table    = otu.culi,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs.tax4fun,
                               auto_tidy    = T)
dataset.keg.warb <- microtable$new(
                               sample_table = samples.warb,
                               otu_table    = otu.warb,
                               tax_table    = tax.table,
                               phylo_tree   = tax.tree,
                               rep_fasta    = rep.seqs.tax4fun,
                               auto_tidy    = T)

dataset.keg.main$rarefy_samples(method = "SRS", sample.size = 7000)
dataset.keg.culi$rarefy_samples(method = "SRS", sample.size = 7000)
dataset.keg.warb$rarefy_samples(method = "SRS", sample.size = 7000)

dataset.keg.main$cal_abund()
dataset.keg.culi$cal_abund()
dataset.keg.warb$cal_abund()

dataset.keg.main$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)
dataset.keg.culi$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)
dataset.keg.warb$filter_taxa(rel_abund = 0.0001, freq = 0.02, include_lowest = TRUE)

keg.main <- trans_func$new(dataset.keg.main)
keg.culi <- trans_func$new(dataset.keg.culi)
keg.warb <- trans_func$new(dataset.keg.warb)
```

```{r}
dir.create(paste0(params$kegg, "test_prediction"))
```

#### Create Functional Profiles

>Note that as of 10/16/2024 I am increasing the min reference ID threshold from 90% to 97%. Now that our sample size has grown we can narrow our analysis to stricter 

```{r}
keg.main$cal_tax4fun2(
                      blast_tool_path           =paste0(params$kegg, "ncbi-blast-2.16.0+/bin"),
                      path_to_reference_data    =paste0(params$kegg, "Tax4Fun2_ReferenceData_v2"),
                      database_mode             = "Ref99NR", 
                      path_to_temp_folder       = paste0(params$kegg, "test_prediction"),
                      min_identity_to_reference = 97,
                      num_threads               = parallel::detectCores() - 1)
```


```{r}
keg.culi$cal_tax4fun2(
                      blast_tool_path           =paste0(params$kegg, "ncbi-blast-2.16.0+/bin"),
                      path_to_reference_data    =paste0(params$kegg, "Tax4Fun2_ReferenceData_v2"),
                      database_mode             = "Ref99NR", 
                      path_to_temp_folder       = paste0(params$kegg, "test_prediction"),
                      min_identity_to_reference = 97,
                      num_threads               = parallel::detectCores() - 1)
keg.warb$cal_tax4fun2(
                      blast_tool_path           =paste0(params$kegg, "ncbi-blast-2.16.0+/bin"),
                      path_to_reference_data    =paste0(params$kegg, "Tax4Fun2_ReferenceData_v2"),
                      database_mode             = "Ref99NR", 
                      path_to_temp_folder       = paste0(params$kegg, "test_prediction"),
                      min_identity_to_reference = 97,
                      num_threads               = parallel::detectCores() - 1)
```

#### Estimate Functional Redundancy

```{r cache=TRUE}
keg.main$cal_tax4fun2_FRI()
keg.culi$cal_tax4fun2_FRI()
keg.warb$cal_tax4fun2_FRI()
```

#### Converting to Dataset Objects

```{r}
data(Tax4Fun2_KEGG)

keg.data.main <- microtable$new(   
                          otu_table    = keg.main$res_tax4fun2_pathway, 
                          tax_table    = Tax4Fun2_KEGG$ptw_desc, 
                          sample_table = dataset.main$sample_table)
keg.data.culi <- microtable$new(   
                          otu_table    = keg.culi$res_tax4fun2_pathway, 
                          tax_table    = Tax4Fun2_KEGG$ptw_desc, 
                          sample_table = dataset.culi$sample_table)
keg.data.warb <- microtable$new(   
                          otu_table    = keg.warb$res_tax4fun2_pathway, 
                          tax_table    = Tax4Fun2_KEGG$ptw_desc, 
                          sample_table = dataset.warb$sample_table)

keg.data.main$tidy_dataset()
keg.data.culi$tidy_dataset()
keg.data.warb$tidy_dataset()
keg.data.main$cal_abund()
keg.data.culi$cal_abund()
keg.data.warb$cal_abund()
keg.data.main$cal_betadiv(method = "canberra")
keg.data.culi$cal_betadiv(method = "canberra")
keg.data.warb$cal_betadiv(method = "canberra")
```

#### Export Datasets for Further Analysis

```{r eval=FALSE}
keg.data.main$save_table(paste0(params$local, "dataframes/keg_dataset_main"  ), sep = "\t")
keg.data.culi$save_table(paste0(params$local, "dataframes/keg_dataset_culi"  ), sep = "\t")
keg.data.warb$save_table(paste0(params$local, "dataframes/keg_dataset_warb"  ), sep = "\t")
```


##### Table Version of Basic Stats

```{r}
  keg.data.main$save_abund(paste0(params$local, "dataframes/keg_abund_main"  ), sep = "\t")
  keg.data.culi$save_abund(paste0(params$local, "dataframes/keg_abund_culi"  ), sep = "\t")
  keg.data.warb$save_abund(paste0(params$local, "dataframes/keg_abund_warb"  ), sep = "\t")
keg.data.main$save_betadiv(paste0(params$local, "dataframes/keg_betadiv_main"))
keg.data.culi$save_betadiv(paste0(params$local, "dataframes/keg_betadiv_culi"))
keg.data.warb$save_betadiv(paste0(params$local, "dataframes/keg_betadiv_warb"))
```

# Next Steps
>Now you should proceed to the Analysis Workflow, where you will import some of the data you compiled and saved here to begin analyzing relationships and trends and creating graphics.