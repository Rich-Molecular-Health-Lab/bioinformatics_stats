---
title: "Constructing Metadata Files"
author: "Alicia Rich"
output:
  html_document:
    toc: true
    toc_location: "before"
    toc_depth: 4
    number_sections: false
    toc_float: true
    code_folding: "hide"
    fig_caption: true
editor_options: 
  chunk_output_type: inline
params:
  sampleset: "loris"
                     
---


# Script

The chunk below is duplicated above. I am adding it here to run the full setup chunk again in case you skipped past the intro material:

```{r, include = F}
global             <- config::get(config = "default")

source(paste0(global$setup))
source(paste0(swan$functions))

seqruns <- list(
  loris     = as.list(paste0("hdz", 1:18)),
  marmoset  = as.list(sprintf("cm%03d", 1:10))
)

subject_list <- keep_at(subjects, paste0(params$sampleset)) %>% list_flatten(name_spec = "{inner}")
path               <- config::get(config = params$sampleset)


```

## Sample Inventory

First I will read in the sample inventory, which is actually already a metadata table itself. We just want to match some more interesting variables to it.

```{r}
samples <- read.table(path$compilation, header = T, sep = "\t") %>%
            mutate(CollectionDate = ymd(CollectionDate))
```




## Construct Variables

The purpose of a metadata file is to organize the potential independent or predictor variables for your analysis into a single table with one row per SampleID. Then, when you produce a set of potential dependent or outcome values, you organize those into a similar structure with the SampleIDs organized rowwise to streamline the process of matching predictor variables to outcome variables by SampleID. It's good practice to keep as much of your information in one tidy table as possible so that you can keep pulling from that source to further filter, wrangle and analyze without losing track of different versions and datasets over time. That means you should brainstorm as many possible predictor variables you might use in downstream analysis as possible and organize them into one tidy table where each SampleID is matched to a value for every variable. You will end up ignoring most of these variables as you construct individual tests and visuals later, so consider this simply a rough draft of your information of interest. I am going to do this with the Pygmy Loris dataset for this tutorial. You may have a very different set of variables to organize for your own project.  

### Organizing Dates

The toughest variables to match and wrange in R are often dates, especially when you are dealing with both states (start and end dates for intervals) and events (single dates). For studies like this one based around daily sample collection, I find it easiest to start by populating a blank dataframe with one row per sample collection day. Then I match date-based variables to this dataframe and join my SampleIDs to it.

```{r}
study_days <- tibble(Date = seq.Date(from = ymd(path$day1), to = ymd(path$last), by = "day"))
```

Now I can begin filling this in with date-based variables.


```{r}
samples.by.day <- samples %>%
  select(SampleID,
         Date = CollectionDate,
         Subject) %>%
  filter(!is.na(Date)) %>%
  full_join(samples.by.day, by = join_by(Date)) %>%
  nest(.by = c("Date", "Subject"), .key = "Samples") %>%
  pivot_wider(id_cols     = Date,
              names_from  = Subject,
              values_from = Samples) %>%
  select(-c("NA")) %>%
  mutate(study_day = row_number()) %>% relocate(study_day)
```

### Diet

We have only two subjects in this dataset: Culi and Warble. Culi underwent a series of diet trials that we organized into start and end dates, while Warble maintained a baseline diet throughout our study.

```{r}
# steroid doses are calculated as mg/day while probiotic and oatgel values are binary (included = 1, none = 0)
diet_trials   <- list(
baseline      = tibble(steroid = 0.00, probiotic = 0, oatgel = 0),
option1       = tibble(steroid = 0.00, probiotic = 1, oatgel = 0),
option2       = tibble(steroid = 0.00, probiotic = 1, oatgel = 1),
option3       = tibble(steroid = 0.10, probiotic = 0, oatgel = 0),
option4       = tibble(steroid = 0.20, probiotic = 0, oatgel = 0),
option5       = tibble(steroid = 0.20, probiotic = 1, oatgel = 1),
option6       = tibble(steroid = 0.20, probiotic = 0, oatgel = 1),
option7       = tibble(steroid = 0.10, probiotic = 1, oatgel = 1),
option8       = tibble(steroid = 0.05, probiotic = 1, oatgel = 1)
)
```


```{r}
# optionals are foods that are routinely cycled in and out of daily diets to maximize variety. here I am only noting those optionals which were permanently removed from culi's diet at a certain date in an attempt to reduce GI inflammation.
optionals <- list(
  tomatoes   = tibble(start_day = ymd(path$day1), last_day = ymd("2024-02-19")),
  califlower = tibble(start_day = ymd(path$day1), last_day = ymd("2024-01-22")),
  broccoli   = tibble(start_day = ymd(path$day1), last_day = ymd("2024-01-22"))
)

culi.schedule <- tribble(
~start_day,        ~last_day        , ~diet,
ymd(loris$day1)  , ymd("2023-11-01"), diet_trials$baseline,
ymd("2023-11-02"), ymd("2023-11-22"), diet_trials$option1,
ymd("2023-11-23"), ymd("2023-12-13"), diet_trials$option2,
ymd("2023-12-14"), ymd("2024-01-03"), diet_trials$option3,
ymd("2024-01-04"), ymd("2024-01-24"), diet_trials$option4,
ymd("2024-01-25"), ymd("2024-02-19"), diet_trials$option5,
ymd("2024-02-20"), ymd("2024-02-22"), diet_trials$option6,
ymd("2024-02-23"), ymd("2024-05-14"), diet_trials$option7,
ymd("2024-05-15"), ymd("2024-12-01"), diet_trials$option8
)

warble.schedule <- tribble(
~start_day,        ~last_day        , ~diet,
ymd(path$day1)  , ymd(path$last)  , diet_trials$baseline  
) %>%
  rowwise() %>%
  mutate(Date = list(seq.Date(from = start_day, to = last_day, by = "day"))) %>%
  unnest(Date) %>%
  select(Date, 
         warble_diet = diet)
```

```{r}
optionals.expand <- optionals %>%
  
```


```{r}
culi.schedule.expand <- culi.schedule %>%
  rowwise() %>%
  mutate(Date = list(seq.Date(from = start_day, to = last_day, by = "day"))) %>%
  unnest(Date) %>%
  select(Date, 
         culi_diet = diet)


```





